{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFL Bike data prep\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data prep as part of my MSc thesis, \"Using machine learning to analyse and predict Transport for London bike sharing habits in the post COVID-19 era\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code for downloading the data has been adopted from [Markus Hauru's](https://github.com/mhauru) analysis, 'Predicting Boris Bike usage'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import statsmodels.api as sm\n",
    "from sklearn import linear_model, svm, neighbors, tree\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from timeit import default_timer as timer\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from urllib.parse import urlparse\n",
    "import openpyxl\n",
    "\n",
    "try:\n",
    "    import xlrd\n",
    "except Exception as e:\n",
    "    msg = (\n",
    "        \"Please install the package xlrd: `pip install --user xlrd`\"\n",
    "        \"It's an optional requirement for pandas, and we'll be needing it.\"\n",
    "    )\n",
    "    print(msg)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EMoses\\AppData\\Local\\Temp\\ipykernel_2668\\99259732.py:4: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats(\"svg\")\n"
     ]
    }
   ],
   "source": [
    "# For pretty and exportable matplotlib plots.\n",
    "# If you are running this yourself and want interactivity,\n",
    "# try `%matplotlib widget` instead.\n",
    "set_matplotlib_formats(\"svg\")\n",
    "%matplotlib inline\n",
    "# %matplotlib widget\n",
    "# Set a consistent plotting style across the notebook using Seaborn.\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"notebook\")\n",
    "# Make pandas cooperate with pyplot\n",
    "pd.plotting.register_matplotlib_converters()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Processing and cleaning the bike data\n",
    "Before getting anywhere with it, we'll need to process the bike data quite a bit. The data comes in CSV files, each of which covers a period of time. Up first, we need to download the data from the TfL website. If you are running this code yourself, here's a script that does that. Be warned though, it's almost seven gigs of data. You can run it repeatedly, and it'll only download data that it doesn't have already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikefolder = \"data/bikes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(datafolder, url, verbosity=0):\n",
    "    \"\"\"Download the data from the given URL into the datafolder, unless it's\n",
    "    already there. Return path to downloaded file.\n",
    "    \"\"\"\n",
    "    # data folder variable for where the folder for where the downloaded file should be stores \n",
    "    # using the path() function to converted the data folder string into a path\n",
    "    datafolder = Path(datafolder)\n",
    "    datafolder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # using the url parse function to extract the file from the url and create a filepath for it to be stored\n",
    "    a = urlparse(url)\n",
    "    filename = Path(os.path.basename(a.path))\n",
    "    filepath = datafolder / filename\n",
    "    # Don't redownload if we already have this file.\n",
    "    if filepath.exists():\n",
    "        if verbosity > 1:\n",
    "            print(\"Already have {}\".format(filename))\n",
    "    else:\n",
    "        if verbosity > 0:\n",
    "            print(\"Downloading {}\".format(filename))\n",
    "        # sends a GET request to the URL using the requests module and raises an exception if there is an error\n",
    "        rqst = requests.get(url)\n",
    "        rqst.raise_for_status()\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            f.write(rqst.content)\n",
    "    return filepath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust whether to print progress reports of the downloads.\n",
    "# verbosity=0 is silence, verbosity=1 reports only when actually doing things,\n",
    "# verbosity>1 also reports when there's nothing to do.\n",
    "verbosity = 1\n",
    "\n",
    "# Most files are individual CSV files, listed in bike_data_urls.txt. Download them.\n",
    "urlsfile = \"data/bikes/bike_data_urls.txt\"\n",
    "with open(urlsfile, \"r\") as f:\n",
    "    urls = f.read().splitlines()\n",
    "# There are a few comments in the file, marked by lines starting with #.\n",
    "# Filter them out.\n",
    "urls = [u for u in urls if u[0] != \"#\"]\n",
    "for url in urls:\n",
    "    download_file(bikefolder, url, verbosity)\n",
    "\n",
    "# The early years come in zips. Download and unzip them.\n",
    "zipsfolder = Path(\"data/bikes/bikezips\")\n",
    "bikezipurls = [\n",
    "    \"https://cycling.data.tfl.gov.uk/usage-stats/cyclehireusagestats-2012.zip\",\n",
    "    \"https://cycling.data.tfl.gov.uk/usage-stats/cyclehireusagestats-2013.zip\",\n",
    "    \"https://cycling.data.tfl.gov.uk/usage-stats/cyclehireusagestats-2014.zip\",\n",
    "    \"https://cycling.data.tfl.gov.uk/usage-stats/2015TripDatazip.zip\",\n",
    "    \"https://cycling.data.tfl.gov.uk/usage-stats/2016TripDataZip.zip\",\n",
    "]\n",
    "# A list of CSV files that are already there. Only unzip if some of the files\n",
    "# in the zip aren't present already.\n",
    "current_csvs = sorted(os.listdir(bikefolder))\n",
    "for url in bikezipurls:\n",
    "    zippath = download_file(zipsfolder, url, verbosity)\n",
    "    with zipfile.ZipFile(zippath, \"r\") as z:\n",
    "        namelist = z.namelist()\n",
    "        has_been_extracted = any(name not in current_csvs for name in namelist)\n",
    "        if has_been_extracted:\n",
    "            if verbosity > 0:\n",
    "                print(\"Unzipping {}\".format(zippath))\n",
    "            z.extractall(bikefolder)\n",
    "        else:\n",
    "            if verbosity > 1:\n",
    "                print(\"{} has already been extracted.\".format(zippath))\n",
    "\n",
    "# Finally, there's an odd one out: One week's data comes in as an .xlsx.\n",
    "# Download it and use pandas to convert it to csv.\n",
    "xlsxurl = \"https://cycling.data.tfl.gov.uk/usage-stats/49JourneyDataExtract15Mar2017-21Mar2017.xlsx\"\n",
    "xlsxfile = download_file(bikefolder, xlsxurl)\n",
    "csvfile = xlsxfile.with_suffix(\".csv\")\n",
    "if not csvfile.exists():\n",
    "    if verbosity > 0:\n",
    "        print(\"Converting .xlsx to .csv.\")\n",
    "    pd.read_excel(xlsxfile).to_csv(csvfile, date_format=\"%d/%m/%Y %H:%M:%S\")\n",
    "else:\n",
    "    if verbosity > 1:\n",
    "        print(\"Already have {}\".format(csvfile))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we have now lists on each line of the CSV file a single bike trip, with starting point and time, end point and time, and things like bike ID number. Here's an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rental Id</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Bike Id</th>\n",
       "      <th>End Date</th>\n",
       "      <th>EndStation Id</th>\n",
       "      <th>EndStation Name</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>StartStation Id</th>\n",
       "      <th>StartStation Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62857677</td>\n",
       "      <td>3780.0</td>\n",
       "      <td>7851</td>\n",
       "      <td>06/03/2017 19:20</td>\n",
       "      <td>43.0</td>\n",
       "      <td>Crawford Street, Marylebone</td>\n",
       "      <td>06/03/2017 18:17</td>\n",
       "      <td>811</td>\n",
       "      <td>Westferry Circus, Canary Wharf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62863035</td>\n",
       "      <td>540.0</td>\n",
       "      <td>4089</td>\n",
       "      <td>06/03/2017 22:17</td>\n",
       "      <td>295.0</td>\n",
       "      <td>Swan Street, The Borough</td>\n",
       "      <td>06/03/2017 22:08</td>\n",
       "      <td>272</td>\n",
       "      <td>Baylis Road, Waterloo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62775896</td>\n",
       "      <td>600.0</td>\n",
       "      <td>4895</td>\n",
       "      <td>02/03/2017 21:27</td>\n",
       "      <td>295.0</td>\n",
       "      <td>Swan Street, The Borough</td>\n",
       "      <td>02/03/2017 21:17</td>\n",
       "      <td>197</td>\n",
       "      <td>Stamford Street, South Bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62747748</td>\n",
       "      <td>420.0</td>\n",
       "      <td>4347</td>\n",
       "      <td>01/03/2017 21:08</td>\n",
       "      <td>295.0</td>\n",
       "      <td>Swan Street, The Borough</td>\n",
       "      <td>01/03/2017 21:01</td>\n",
       "      <td>803</td>\n",
       "      <td>Southwark Street, Bankside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62843939</td>\n",
       "      <td>420.0</td>\n",
       "      <td>3192</td>\n",
       "      <td>06/03/2017 09:28</td>\n",
       "      <td>193.0</td>\n",
       "      <td>Bankside Mix, Bankside</td>\n",
       "      <td>06/03/2017 09:21</td>\n",
       "      <td>197</td>\n",
       "      <td>Stamford Street, South Bank</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rental Id  Duration  Bike Id          End Date  EndStation Id  \\\n",
       "0   62857677    3780.0     7851  06/03/2017 19:20           43.0   \n",
       "1   62863035     540.0     4089  06/03/2017 22:17          295.0   \n",
       "2   62775896     600.0     4895  02/03/2017 21:27          295.0   \n",
       "3   62747748     420.0     4347  01/03/2017 21:08          295.0   \n",
       "4   62843939     420.0     3192  06/03/2017 09:28          193.0   \n",
       "\n",
       "               EndStation Name        Start Date  StartStation Id  \\\n",
       "0  Crawford Street, Marylebone  06/03/2017 18:17              811   \n",
       "1     Swan Street, The Borough  06/03/2017 22:08              272   \n",
       "2     Swan Street, The Borough  02/03/2017 21:17              197   \n",
       "3     Swan Street, The Borough  01/03/2017 21:01              803   \n",
       "4       Bankside Mix, Bankside  06/03/2017 09:21              197   \n",
       "\n",
       "                StartStation Name  \n",
       "0  Westferry Circus, Canary Wharf  \n",
       "1           Baylis Road, Waterloo  \n",
       "2     Stamford Street, South Bank  \n",
       "3      Southwark Street, Bankside  \n",
       "4     Stamford Street, South Bank  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_file  = Path(bikefolder) / Path(\"47JourneyDataExtract01Mar2017-07Mar2017.csv\")\n",
    "pd.read_csv(example_file, encoding=\"ISO-8859-2\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/bikes\\\\01aJourneyDataExtract10Jan16-23Jan16.csv',\n",
       " 'data/bikes\\\\01bJourneyDataExtract24Jan16-06Feb16.csv',\n",
       " 'data/bikes\\\\02aJourneyDataExtract07Feb16-20Feb2016.csv',\n",
       " 'data/bikes\\\\02bJourneyDataExtract21Feb16-05Mar2016.csv',\n",
       " 'data/bikes\\\\03JourneyDataExtract06Mar2016-31Mar2016.csv',\n",
       " 'data/bikes\\\\04JourneyDataExtract01Apr2016-30Apr2016.csv',\n",
       " 'data/bikes\\\\05JourneyDataExtract01May2016-17May2016.csv',\n",
       " 'data/bikes\\\\06JourneyDataExtract18May2016-24May2016.csv',\n",
       " 'data/bikes\\\\07JourneyDataExtract25May2016-31May2016.csv',\n",
       " 'data/bikes\\\\08JourneyDataExtract01Jun2016-07Jun2016.csv',\n",
       " 'data/bikes\\\\09JourneyDataExtract08Jun2016-14Jun2016.csv',\n",
       " 'data/bikes\\\\1. Journey Data Extract 01Jan-05Jan13.csv',\n",
       " 'data/bikes\\\\1. Journey Data Extract 04Jan-31Jan 12.csv',\n",
       " 'data/bikes\\\\1. Journey Data Extract 05Jan14-02Feb14.csv',\n",
       " 'data/bikes\\\\10. Journey Data Extract 18Aug-13Sep13.csv',\n",
       " 'data/bikes\\\\10. Journey Data Extract 21Aug-22 Aug12.csv',\n",
       " 'data/bikes\\\\10a Journey Data Extract 20Sep15-03Oct15.csv',\n",
       " 'data/bikes\\\\10a. Journey Data Extract 14Sep14-27Sep14.csv',\n",
       " 'data/bikes\\\\10b Journey Data Extract 04Oct15-17Oct15.csv',\n",
       " 'data/bikes\\\\10b. Journey Data Extract 28Sep14-11Oct14.csv',\n",
       " 'data/bikes\\\\10JourneyDataExtract15Jun2016-21Jun2016.csv',\n",
       " 'data/bikes\\\\11. Journey Data Extract 14Sep13-12Oct13.csv',\n",
       " 'data/bikes\\\\11. Journey Data Extract 23Aug-25 Aug12.csv',\n",
       " 'data/bikes\\\\11a Journey Data Extract 18Oct15-31Oct15.csv',\n",
       " 'data/bikes\\\\11a. Journey Data Extract 12Oct14-08Nov14.csv',\n",
       " 'data/bikes\\\\11b Journey Data Extract 01Nov15-14Nov15.csv',\n",
       " 'data/bikes\\\\11b. Journey Data Extract 12Oct14-08Nov14.csv',\n",
       " 'data/bikes\\\\11JourneyDataExtract22Jun2016-28Jun2016.csv',\n",
       " 'data/bikes\\\\12. Journey Data Extract 13Oct13-09Nov13.csv',\n",
       " 'data/bikes\\\\12. Journey Data Extract 26Aug-27 Aug12.csv',\n",
       " 'data/bikes\\\\12a Journey Data Extract 15Nov15-27Nov15.csv',\n",
       " 'data/bikes\\\\12a. Journey Data Extract 09Nov14-06Dec14.csv',\n",
       " 'data/bikes\\\\12b Journey Data Extract 28Nov15-12Dec15.csv',\n",
       " 'data/bikes\\\\12b. Journey Data Extract 09Nov14-06Dec14.csv',\n",
       " 'data/bikes\\\\12JourneyDataExtract29Jun2016-05Jul2016.csv',\n",
       " 'data/bikes\\\\13. Journey Data Extract 10Nov13-07Dec13.csv',\n",
       " 'data/bikes\\\\13. Journey Data Extract 28Aug-29 Aug12.csv',\n",
       " 'data/bikes\\\\13a Journey Data Extract 13Dec15-24Dec15.csv',\n",
       " 'data/bikes\\\\13a. Journey Data Extract 07Dec14-21Dec14.csv',\n",
       " 'data/bikes\\\\13b Journey Data Extract 25Dec15-09Jan16.csv',\n",
       " 'data/bikes\\\\13b. Journey Data Extract 22Dec14-03Jan15.csv',\n",
       " 'data/bikes\\\\13JourneyDataExtract06Jul2016-12Jul2016.csv',\n",
       " 'data/bikes\\\\14. Journey Data Extract 08Dec13-04Jan14.csv',\n",
       " 'data/bikes\\\\14. Journey Data Extract 30Aug-31 Aug12.csv',\n",
       " 'data/bikes\\\\142JourneyDataExtract26Dec2018-01Jan2019.csv',\n",
       " 'data/bikes\\\\143JourneyDataExtract02Jan2019-08Jan2019.csv',\n",
       " 'data/bikes\\\\144JourneyDataExtract09Jan2019-15Jan2019.csv',\n",
       " 'data/bikes\\\\145JourneyDataExtract16Jan2019-22Jan2019.csv',\n",
       " 'data/bikes\\\\146JourneyDataExtract23Jan2019-29Jan2019.csv',\n",
       " 'data/bikes\\\\147JourneyDataExtract30Jan2019-05Feb2019.csv',\n",
       " 'data/bikes\\\\148JourneyDataExtract06Feb2019-12Feb2019.csv',\n",
       " 'data/bikes\\\\149JourneyDataExtract13Feb2019-19Feb2019.csv',\n",
       " 'data/bikes\\\\14JourneyDataExtract13Jul2016-19Jul2016.csv',\n",
       " 'data/bikes\\\\15. Journey Data Extract 01Sep-30Sep12.csv',\n",
       " 'data/bikes\\\\150JourneyDataExtract20Feb2019-26Feb2019.csv',\n",
       " 'data/bikes\\\\151JourneyDataExtract27Feb2019-05Mar2019.csv',\n",
       " 'data/bikes\\\\152JourneyDataExtract06Mar2019-12Mar2019.csv',\n",
       " 'data/bikes\\\\153JourneyDataExtract13Mar2019-19Mar2019.csv',\n",
       " 'data/bikes\\\\154JourneyDataExtract20Mar2019-26Mar2019.csv',\n",
       " 'data/bikes\\\\155JourneyDataExtract27Mar2019-02Apr2019.csv',\n",
       " 'data/bikes\\\\156JourneyDataExtract03Apr2019-09Apr2019.csv',\n",
       " 'data/bikes\\\\157JourneyDataExtract10Apr2019-16Apr2019.csv',\n",
       " 'data/bikes\\\\158JourneyDataExtract17Apr2019-23Apr2019.csv',\n",
       " 'data/bikes\\\\159JourneyDataExtract24Apr2019-30Apr2019.csv',\n",
       " 'data/bikes\\\\15JourneyDataExtract20Jul2016-26Jul2016.csv',\n",
       " 'data/bikes\\\\16. Journey Data Extract 01Oct-31Oct12.csv',\n",
       " 'data/bikes\\\\160JourneyDataExtract01May2019-07May2019.csv',\n",
       " 'data/bikes\\\\161JourneyDataExtract08May2019-14May2019.csv',\n",
       " 'data/bikes\\\\162JourneyDataExtract15May2019-21May2019.csv',\n",
       " 'data/bikes\\\\163JourneyDataExtract22May2019-28May2019.csv',\n",
       " 'data/bikes\\\\164JourneyDataExtract29May2019-04Jun2019.csv',\n",
       " 'data/bikes\\\\165JourneyDataExtract05Jun2019-11Jun2019.csv',\n",
       " 'data/bikes\\\\166JourneyDataExtract12Jun2019-18Jun2019.csv',\n",
       " 'data/bikes\\\\167JourneyDataExtract19Jun2019-25Jun2019.csv',\n",
       " 'data/bikes\\\\168JourneyDataExtract26Jun2019-02Jul2019.csv',\n",
       " 'data/bikes\\\\169JourneyDataExtract03Jul2019-09Jul2019.csv',\n",
       " 'data/bikes\\\\16JourneyDataExtract27Jul2016-02Aug2016.csv',\n",
       " 'data/bikes\\\\17. Journey Data Extract 01Nov-30Nov12.csv',\n",
       " 'data/bikes\\\\170JourneyDataExtract10Jul2019-16Jul2019.csv',\n",
       " 'data/bikes\\\\171JourneyDataExtract17Jul2019-23Jul2019.csv',\n",
       " 'data/bikes\\\\172JourneyDataExtract24Jul2019-30Jul2019.csv',\n",
       " 'data/bikes\\\\173JourneyDataExtract31Jul2019-06Aug2019.csv',\n",
       " 'data/bikes\\\\174JourneyDataExtract07Aug2019-13Aug2019.csv',\n",
       " 'data/bikes\\\\175JourneyDataExtract14Aug2019-20Aug2019.csv',\n",
       " 'data/bikes\\\\176JourneyDataExtract21Aug2019-27Aug2019.csv',\n",
       " 'data/bikes\\\\177JourneyDataExtract28Aug2019-03Sep2019.csv',\n",
       " 'data/bikes\\\\178JourneyDataExtract04Sep2019-10Sep2019.csv',\n",
       " 'data/bikes\\\\179JourneyDataExtract11Sep2019-17Sep2019.csv',\n",
       " 'data/bikes\\\\17JourneyDataExtract03Aug2016-09Aug2016.csv',\n",
       " 'data/bikes\\\\18. Journey Data Extract 01Dec-31Dec12.csv',\n",
       " 'data/bikes\\\\180JourneyDataExtract18Sep2019-24Sep2019.csv',\n",
       " 'data/bikes\\\\181JourneyDataExtract25Sep2019-01Oct2019.csv',\n",
       " 'data/bikes\\\\182JourneyDataExtract02Oct2019-08Oct2019.csv',\n",
       " 'data/bikes\\\\183JourneyDataExtract09Oct2019-15Oct2019.csv',\n",
       " 'data/bikes\\\\184JourneyDataExtract16Oct2019-22Oct2019.csv',\n",
       " 'data/bikes\\\\185JourneyDataExtract23Oct2019-29Oct2019.csv',\n",
       " 'data/bikes\\\\186JourneyDataExtract30Oct2019-05Nov2019.csv',\n",
       " 'data/bikes\\\\187JourneyDataExtract06Nov2019-12Nov2019.csv',\n",
       " 'data/bikes\\\\188JourneyDataExtract13Nov2019-19Nov2019.csv',\n",
       " 'data/bikes\\\\189JourneyDataExtract20Nov2019-26Nov2019.csv',\n",
       " 'data/bikes\\\\18JourneyDataExtract10Aug2016-16Aug2016.csv',\n",
       " 'data/bikes\\\\190JourneyDataExtract27Nov2019-03Dec2019.csv',\n",
       " 'data/bikes\\\\191JourneyDataExtract04Dec2019-10Dec2019.csv',\n",
       " 'data/bikes\\\\192JourneyDataExtract11Dec2019-17Dec2019.csv',\n",
       " 'data/bikes\\\\193JourneyDataExtract18Dec2019-24Dec2019.csv',\n",
       " 'data/bikes\\\\194JourneyDataExtract25Dec2019-31Dec2019.csv',\n",
       " 'data/bikes\\\\19JourneyDataExtract17Aug2016-23Aug2016.csv',\n",
       " 'data/bikes\\\\1a.JourneyDataExtract04Jan15-17Jan15.csv',\n",
       " 'data/bikes\\\\1b.JourneyDataExtract18Jan15-31Jan15.csv',\n",
       " 'data/bikes\\\\2. Journey Data Extract 03Feb14-01Mar14.csv',\n",
       " 'data/bikes\\\\2. Journey Data Extract 06Jan-02Feb13.csv',\n",
       " 'data/bikes\\\\2. Journey Data Extract_01Feb-29Feb 12.csv',\n",
       " 'data/bikes\\\\20JourneyDataExtract24Aug2016-30Aug2016.csv',\n",
       " 'data/bikes\\\\21JourneyDataExtract31Aug2016-06Sep2016.csv',\n",
       " 'data/bikes\\\\22JourneyDataExtract07Sep2016-13Sep2016.csv',\n",
       " 'data/bikes\\\\23JourneyDataExtract14Sep2016-20Sep2016.csv',\n",
       " 'data/bikes\\\\24JourneyDataExtract21Sep2016-27Sep2016.csv',\n",
       " 'data/bikes\\\\25JourneyDataExtract28Sep2016-04Oct2016.csv',\n",
       " 'data/bikes\\\\26JourneyDataExtract05Oct2016-11Oct2016.csv',\n",
       " 'data/bikes\\\\27JourneyDataExtract12Oct2016-18Oct2016.csv',\n",
       " 'data/bikes\\\\28JourneyDataExtract19Oct2016-25Oct2016.csv',\n",
       " 'data/bikes\\\\298JourneyDataExtract29Dec2021-04Jan2022.csv',\n",
       " 'data/bikes\\\\299JourneyDataExtract05Jan2022-11Jan2022.csv',\n",
       " 'data/bikes\\\\29JourneyDataExtract26Oct2016-01Nov2016.csv',\n",
       " 'data/bikes\\\\2a.JourneyDataExtract01Feb15-14Feb15.csv',\n",
       " 'data/bikes\\\\2b.JourneyDataExtract15Feb15-28Feb15.csv',\n",
       " 'data/bikes\\\\3. Journey Data Extract 02Mar14-31Mar14.csv',\n",
       " 'data/bikes\\\\3. Journey Data Extract 03Feb-02Mar13.csv',\n",
       " 'data/bikes\\\\3. Journey Data Extract_01Mar-31Mar12.csv',\n",
       " 'data/bikes\\\\300JourneyDataExtract12Jan2022-18Jan2022.csv',\n",
       " 'data/bikes\\\\301JourneyDataExtract19Jan2022-25Jan2022.csv',\n",
       " 'data/bikes\\\\302JourneyDataExtract26Jan2022-01Feb2022.csv',\n",
       " 'data/bikes\\\\303JourneyDataExtract02Feb2022-08Feb2022.csv',\n",
       " 'data/bikes\\\\304JourneyDataExtract09Feb2022-15Feb2022.csv',\n",
       " 'data/bikes\\\\305JourneyDataExtract16Feb2022-22Feb2022.csv',\n",
       " 'data/bikes\\\\306JourneyDataExtract23Feb2022-01Mar2022.csv',\n",
       " 'data/bikes\\\\307JourneyDataExtract02Mar2022-08Mar2022.csv',\n",
       " 'data/bikes\\\\308JourneyDataExtract09Mar2022-15Mar2022.csv',\n",
       " 'data/bikes\\\\309JourneyDataExtract16Mar2022-22Mar2022.csv',\n",
       " 'data/bikes\\\\30JourneyDataExtract02Nov2016-08Nov2016.csv',\n",
       " 'data/bikes\\\\310JourneyDataExtract23Mar2022-29Mar2022.csv',\n",
       " 'data/bikes\\\\311JourneyDataExtract30Mar2022-05Apr2022.csv',\n",
       " 'data/bikes\\\\312JourneyDataExtract06Apr2022-12Apr2022.csv',\n",
       " 'data/bikes\\\\313JourneyDataExtract13Apr2022-19Apr2022.csv',\n",
       " 'data/bikes\\\\314JourneyDataExtract20Apr2022-26Apr2022.csv',\n",
       " 'data/bikes\\\\315JourneyDataExtract27Apr2022-03May2022.csv',\n",
       " 'data/bikes\\\\316JourneyDataExtract04May2022-10May2022.csv',\n",
       " 'data/bikes\\\\317JourneyDataExtract11May2022-17May2022.csv',\n",
       " 'data/bikes\\\\318JourneyDataExtract18May2022-24May2022.csv',\n",
       " 'data/bikes\\\\319JourneyDataExtract25May2022-31May2022.csv',\n",
       " 'data/bikes\\\\31JourneyDataExtract09Nov2016-15Nov2016.csv',\n",
       " 'data/bikes\\\\320JourneyDataExtract01Jun2022-07Jun2022.csv',\n",
       " 'data/bikes\\\\321JourneyDataExtract08Jun2022-14Jun2022.csv',\n",
       " 'data/bikes\\\\322JourneyDataExtract15Jun2022-21Jun2022.csv',\n",
       " 'data/bikes\\\\323JourneyDataExtract22Jun2022-28Jun2022.csv',\n",
       " 'data/bikes\\\\324JourneyDataExtract29Jun2022-05Jul2022.csv',\n",
       " 'data/bikes\\\\325JourneyDataExtract06Jul2022-12Jul2022.csv',\n",
       " 'data/bikes\\\\326JourneyDataExtract13Jul2022-19Jul2022.csv',\n",
       " 'data/bikes\\\\327JourneyDataExtract20Jul2022-26Jul2022.csv',\n",
       " 'data/bikes\\\\328JourneyDataExtract27Jul2022-02Aug2022.csv',\n",
       " 'data/bikes\\\\329JourneyDataExtract03Aug2022-09Aug2022.csv',\n",
       " 'data/bikes\\\\32JourneyDataExtract16Nov2016-22Nov2016.csv',\n",
       " 'data/bikes\\\\330JourneyDataExtract10Aug2022-16Aug2022.csv',\n",
       " 'data/bikes\\\\331JourneyDataExtract17Aug2022-23Aug2022.csv',\n",
       " 'data/bikes\\\\332JourneyDataExtract24Aug2022-30Aug2022.csv',\n",
       " 'data/bikes\\\\333JourneyDataExtract31Aug2022-06Sep2022.csv',\n",
       " 'data/bikes\\\\334JourneyDataExtract07Sep2022-11Sep2022.csv',\n",
       " 'data/bikes\\\\335JourneyDataExtract12Sep2022-18Sep2022.csv',\n",
       " 'data/bikes\\\\336JourneyDataExtract19Sep2022-25Sep2022.csv',\n",
       " 'data/bikes\\\\337JourneyDataExtract26Sep2022-02Oct2022.csv',\n",
       " 'data/bikes\\\\338JourneyDataExtract03Oct2022-09Oct2022.csv',\n",
       " 'data/bikes\\\\339JourneyDataExtract10Oct2022-16Oct2022.csv',\n",
       " 'data/bikes\\\\33JourneyDataExtract23Nov2016-29Nov2016.csv',\n",
       " 'data/bikes\\\\340JourneyDataExtract17Oct2022-23Oct2022.csv',\n",
       " 'data/bikes\\\\341JourneyDataExtract24Oct2022-30Oct2022.csv',\n",
       " 'data/bikes\\\\342JourneyDataExtract31Oct2022-06Nov2022.csv',\n",
       " 'data/bikes\\\\343JourneyDataExtract07Nov2022-13Nov2022.csv',\n",
       " 'data/bikes\\\\344JourneyDataExtract14Nov2022-20Nov2022.csv',\n",
       " 'data/bikes\\\\345JourneyDataExtract21Nov2022-27Nov2022.csv',\n",
       " 'data/bikes\\\\346JourneyDataExtract28Nov2022-04Dec2022.csv',\n",
       " 'data/bikes\\\\347JourneyDataExtract05Dec2022-11Dec2022.csv',\n",
       " 'data/bikes\\\\348JourneyDataExtract12Dec2022-18Dec2022.csv',\n",
       " 'data/bikes\\\\349JourneyDataExtract19Dec2022-25Dec2022.csv',\n",
       " 'data/bikes\\\\34JourneyDataExtract30Nov2016-06Dec2016.csv',\n",
       " 'data/bikes\\\\350JourneyDataExtract26Dec2022-01Jan2023.csv',\n",
       " 'data/bikes\\\\35JourneyDataExtract07Dec2016-13Dec2016.csv',\n",
       " 'data/bikes\\\\36JourneyDataExtract14Dec2016-20Dec2016.csv',\n",
       " 'data/bikes\\\\37JourneyDataExtract21Dec2016-27Dec2016.csv',\n",
       " 'data/bikes\\\\38JourneyDataExtract28Dec2016-03Jan2017.csv',\n",
       " 'data/bikes\\\\39JourneyDataExtract04Jan2017-10Jan2017.csv',\n",
       " 'data/bikes\\\\3a.JourneyDataExtract01Mar15-15Mar15.csv',\n",
       " 'data/bikes\\\\3b.JourneyDataExtract16Mar15-31Mar15.csv',\n",
       " 'data/bikes\\\\4. Journey Data Extract 01Apr14-26Apr14.csv',\n",
       " 'data/bikes\\\\4. Journey Data Extract 03Mar-31Mar13.csv',\n",
       " 'data/bikes\\\\4. Journey Data Extract_1Apr-28Apr12.csv',\n",
       " 'data/bikes\\\\40JourneyDataExtract11Jan2017-17Jan2017.csv',\n",
       " 'data/bikes\\\\41JourneyDataExtract18Jan2017-24Jan2017.csv',\n",
       " 'data/bikes\\\\42JourneyDataExtract25Jan2017-31Jan2017.csv',\n",
       " 'data/bikes\\\\43JourneyDataExtract01Feb2017-07Feb2017.csv',\n",
       " 'data/bikes\\\\44JourneyDataExtract08Feb2017-14Feb2017.csv',\n",
       " 'data/bikes\\\\45JourneyDataExtract15Feb2017-21Feb2017.csv',\n",
       " 'data/bikes\\\\46JourneyDataExtract22Feb2017-28Feb2017.csv',\n",
       " 'data/bikes\\\\47JourneyDataExtract01Mar2017-07Mar2017.csv',\n",
       " 'data/bikes\\\\48JourneyDataExtract08Mar2017-14Mar2017.csv',\n",
       " 'data/bikes\\\\49JourneyDataExtract15Mar2017-21Mar2017.csv',\n",
       " 'data/bikes\\\\4a.JourneyDataExtract01Apr15-16Apr15.csv',\n",
       " 'data/bikes\\\\4b.JourneyDataExtract 17Apr15-02May15.csv',\n",
       " 'data/bikes\\\\5. Journey Data Extract 01Apr-27Apr13.csv',\n",
       " 'data/bikes\\\\5. Journey Data Extract 27Apr14-24May14.csv',\n",
       " 'data/bikes\\\\5. Journey Data Extract_29Apr-26May12.csv',\n",
       " 'data/bikes\\\\50 Journey Data Extract 22Mar2017-28Mar2017.csv',\n",
       " 'data/bikes\\\\51 Journey Data Extract 29Mar2017-04Apr2017.csv',\n",
       " 'data/bikes\\\\52 Journey Data Extract 05Apr2017-11Apr2017.csv',\n",
       " 'data/bikes\\\\53JourneyDataExtract12Apr2017-18Apr2017.csv',\n",
       " 'data/bikes\\\\54JourneyDataExtract19Apr2017-25Apr2017.csv',\n",
       " 'data/bikes\\\\55JourneyData Extract26Apr2017-02May2017.csv',\n",
       " 'data/bikes\\\\56JourneyDataExtract 03May2017-09May2017.csv',\n",
       " 'data/bikes\\\\57JourneyDataExtract10May2017-16May2017.csv',\n",
       " 'data/bikes\\\\5a.JourneyDataExtract03May15-16May15.csv',\n",
       " 'data/bikes\\\\5b.JourneyDataExtract17May15-30May15.csv',\n",
       " 'data/bikes\\\\6. Journey Data Extract 25May14-21Jun14.csv',\n",
       " 'data/bikes\\\\6. Journey Data Extract 28Apr-25May13.csv',\n",
       " 'data/bikes\\\\6. Journey Data Extract_27May-23Jun12.csv',\n",
       " 'data/bikes\\\\6aJourneyDataExtract31May15-12Jun15.csv',\n",
       " 'data/bikes\\\\6bJourneyDataExtract13Jun15-27Jun15.csv',\n",
       " 'data/bikes\\\\7. Journey Data Extract 22Jun14-19Jul14.csv',\n",
       " 'data/bikes\\\\7. Journey Data Extract 26May-22Jun13.csv',\n",
       " 'data/bikes\\\\7. Journey Data Extract_24Jun-21Jul12.csv',\n",
       " 'data/bikes\\\\7a.JourneyDataExtract28Jun15-11Jul15.csv',\n",
       " 'data/bikes\\\\7b.JourneyDataExtract12Jul15-25Jul15.csv',\n",
       " 'data/bikes\\\\8. Journey Data Extract 22Jul-18Aug12.csv',\n",
       " 'data/bikes\\\\8. Journey Data Extract 23Jun-20Jul13.csv',\n",
       " 'data/bikes\\\\8a Journey Data Extract 20Jul14-31Jul14.csv',\n",
       " 'data/bikes\\\\8aJourneyDataExtract26Jul15-07Aug15.csv',\n",
       " 'data/bikes\\\\8b Journey Data Extract 01Aug14-16Aug14.csv',\n",
       " 'data/bikes\\\\8bJourneyData Extract 08Aug15-22Aug15.csv',\n",
       " 'data/bikes\\\\9. Journey Data Extract 19Aug-20 Aug12.csv',\n",
       " 'data/bikes\\\\9. Journey Data Extract 21Jul-17Aug13.csv',\n",
       " 'data/bikes\\\\9a Journey Data Extract 17Aug14-31Aug14.csv',\n",
       " 'data/bikes\\\\9a-Journey-Data-Extract-23Aug15-05Sep15.csv',\n",
       " 'data/bikes\\\\9b Journey Data Extract 01Sep14-13Sep14.csv',\n",
       " 'data/bikes\\\\9b-Journey-Data-Extract-06Sep15-19Sep15.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob \n",
    "\n",
    "# using glob to list all the csv file in the bikefolder filepath\n",
    "all_csv = glob(bikefolder+str('/*.csv'))\n",
    "all_csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2019 data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of csv files that contain '2019' and '2022' respectively\n",
    "csv_2019 = [item for item in all_csv if '2019' in item]\n",
    "csv_2022 = [item for item in all_csv if '2022' in item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/bikes\\\\142JourneyDataExtract26Dec2018-01Jan2019.csv',\n",
       " 'data/bikes\\\\143JourneyDataExtract02Jan2019-08Jan2019.csv',\n",
       " 'data/bikes\\\\144JourneyDataExtract09Jan2019-15Jan2019.csv',\n",
       " 'data/bikes\\\\145JourneyDataExtract16Jan2019-22Jan2019.csv',\n",
       " 'data/bikes\\\\146JourneyDataExtract23Jan2019-29Jan2019.csv',\n",
       " 'data/bikes\\\\147JourneyDataExtract30Jan2019-05Feb2019.csv',\n",
       " 'data/bikes\\\\148JourneyDataExtract06Feb2019-12Feb2019.csv',\n",
       " 'data/bikes\\\\149JourneyDataExtract13Feb2019-19Feb2019.csv',\n",
       " 'data/bikes\\\\150JourneyDataExtract20Feb2019-26Feb2019.csv',\n",
       " 'data/bikes\\\\151JourneyDataExtract27Feb2019-05Mar2019.csv',\n",
       " 'data/bikes\\\\152JourneyDataExtract06Mar2019-12Mar2019.csv',\n",
       " 'data/bikes\\\\153JourneyDataExtract13Mar2019-19Mar2019.csv',\n",
       " 'data/bikes\\\\154JourneyDataExtract20Mar2019-26Mar2019.csv',\n",
       " 'data/bikes\\\\155JourneyDataExtract27Mar2019-02Apr2019.csv',\n",
       " 'data/bikes\\\\156JourneyDataExtract03Apr2019-09Apr2019.csv',\n",
       " 'data/bikes\\\\157JourneyDataExtract10Apr2019-16Apr2019.csv',\n",
       " 'data/bikes\\\\158JourneyDataExtract17Apr2019-23Apr2019.csv',\n",
       " 'data/bikes\\\\159JourneyDataExtract24Apr2019-30Apr2019.csv',\n",
       " 'data/bikes\\\\160JourneyDataExtract01May2019-07May2019.csv',\n",
       " 'data/bikes\\\\161JourneyDataExtract08May2019-14May2019.csv',\n",
       " 'data/bikes\\\\162JourneyDataExtract15May2019-21May2019.csv',\n",
       " 'data/bikes\\\\163JourneyDataExtract22May2019-28May2019.csv',\n",
       " 'data/bikes\\\\164JourneyDataExtract29May2019-04Jun2019.csv',\n",
       " 'data/bikes\\\\165JourneyDataExtract05Jun2019-11Jun2019.csv',\n",
       " 'data/bikes\\\\166JourneyDataExtract12Jun2019-18Jun2019.csv',\n",
       " 'data/bikes\\\\167JourneyDataExtract19Jun2019-25Jun2019.csv',\n",
       " 'data/bikes\\\\168JourneyDataExtract26Jun2019-02Jul2019.csv',\n",
       " 'data/bikes\\\\169JourneyDataExtract03Jul2019-09Jul2019.csv',\n",
       " 'data/bikes\\\\170JourneyDataExtract10Jul2019-16Jul2019.csv',\n",
       " 'data/bikes\\\\171JourneyDataExtract17Jul2019-23Jul2019.csv',\n",
       " 'data/bikes\\\\172JourneyDataExtract24Jul2019-30Jul2019.csv',\n",
       " 'data/bikes\\\\173JourneyDataExtract31Jul2019-06Aug2019.csv',\n",
       " 'data/bikes\\\\174JourneyDataExtract07Aug2019-13Aug2019.csv',\n",
       " 'data/bikes\\\\175JourneyDataExtract14Aug2019-20Aug2019.csv',\n",
       " 'data/bikes\\\\176JourneyDataExtract21Aug2019-27Aug2019.csv',\n",
       " 'data/bikes\\\\177JourneyDataExtract28Aug2019-03Sep2019.csv',\n",
       " 'data/bikes\\\\178JourneyDataExtract04Sep2019-10Sep2019.csv',\n",
       " 'data/bikes\\\\179JourneyDataExtract11Sep2019-17Sep2019.csv',\n",
       " 'data/bikes\\\\180JourneyDataExtract18Sep2019-24Sep2019.csv',\n",
       " 'data/bikes\\\\181JourneyDataExtract25Sep2019-01Oct2019.csv',\n",
       " 'data/bikes\\\\182JourneyDataExtract02Oct2019-08Oct2019.csv',\n",
       " 'data/bikes\\\\183JourneyDataExtract09Oct2019-15Oct2019.csv',\n",
       " 'data/bikes\\\\184JourneyDataExtract16Oct2019-22Oct2019.csv',\n",
       " 'data/bikes\\\\185JourneyDataExtract23Oct2019-29Oct2019.csv',\n",
       " 'data/bikes\\\\186JourneyDataExtract30Oct2019-05Nov2019.csv',\n",
       " 'data/bikes\\\\187JourneyDataExtract06Nov2019-12Nov2019.csv',\n",
       " 'data/bikes\\\\188JourneyDataExtract13Nov2019-19Nov2019.csv',\n",
       " 'data/bikes\\\\189JourneyDataExtract20Nov2019-26Nov2019.csv',\n",
       " 'data/bikes\\\\190JourneyDataExtract27Nov2019-03Dec2019.csv',\n",
       " 'data/bikes\\\\191JourneyDataExtract04Dec2019-10Dec2019.csv',\n",
       " 'data/bikes\\\\192JourneyDataExtract11Dec2019-17Dec2019.csv',\n",
       " 'data/bikes\\\\193JourneyDataExtract18Dec2019-24Dec2019.csv',\n",
       " 'data/bikes\\\\194JourneyDataExtract25Dec2019-31Dec2019.csv']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using list comprehension that reads each csv file from the list and gnerators a sequence of dataframes\n",
    "dfs = (pd.read_csv(csv) for csv in csv_2019)\n",
    "\n",
    "# concatenate csvs them into a single DataFrame using pd.concat()\n",
    "# ignore_index=True parameter resets the index of the resulting DataFrame, so that it is a continuous sequence of integers.\n",
    "data_2019 = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10388411, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rental Id</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Bike Id</th>\n",
       "      <th>End Date</th>\n",
       "      <th>EndStation Id</th>\n",
       "      <th>EndStation Name</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>StartStation Id</th>\n",
       "      <th>StartStation Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83252102</td>\n",
       "      <td>720</td>\n",
       "      <td>2077</td>\n",
       "      <td>31/12/2018 19:05</td>\n",
       "      <td>272</td>\n",
       "      <td>Baylis Road, Waterloo</td>\n",
       "      <td>31/12/2018 18:53</td>\n",
       "      <td>94</td>\n",
       "      <td>Bricklayers Arms, Borough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83195883</td>\n",
       "      <td>120</td>\n",
       "      <td>10781</td>\n",
       "      <td>27/12/2018 19:47</td>\n",
       "      <td>93</td>\n",
       "      <td>Cloudesley Road, Angel</td>\n",
       "      <td>27/12/2018 19:45</td>\n",
       "      <td>339</td>\n",
       "      <td>Risinghill Street, Angel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83196070</td>\n",
       "      <td>120</td>\n",
       "      <td>2977</td>\n",
       "      <td>27/12/2018 20:11</td>\n",
       "      <td>339</td>\n",
       "      <td>Risinghill Street, Angel</td>\n",
       "      <td>27/12/2018 20:09</td>\n",
       "      <td>234</td>\n",
       "      <td>Liverpool Road (N1 Centre), Angel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83197932</td>\n",
       "      <td>660</td>\n",
       "      <td>10802</td>\n",
       "      <td>28/12/2018 07:35</td>\n",
       "      <td>282</td>\n",
       "      <td>Royal London Hospital, Whitechapel</td>\n",
       "      <td>28/12/2018 07:24</td>\n",
       "      <td>698</td>\n",
       "      <td>Shoreditch Court, Haggerston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83176351</td>\n",
       "      <td>1380</td>\n",
       "      <td>15749</td>\n",
       "      <td>26/12/2018 11:55</td>\n",
       "      <td>785</td>\n",
       "      <td>Aquatic Centre, Queen Elizabeth Olympic Park</td>\n",
       "      <td>26/12/2018 11:32</td>\n",
       "      <td>783</td>\n",
       "      <td>Monier Road, Hackney Wick</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rental Id  Duration  Bike Id          End Date  EndStation Id  \\\n",
       "0   83252102       720     2077  31/12/2018 19:05            272   \n",
       "1   83195883       120    10781  27/12/2018 19:47             93   \n",
       "2   83196070       120     2977  27/12/2018 20:11            339   \n",
       "3   83197932       660    10802  28/12/2018 07:35            282   \n",
       "4   83176351      1380    15749  26/12/2018 11:55            785   \n",
       "\n",
       "                                EndStation Name        Start Date  \\\n",
       "0                         Baylis Road, Waterloo  31/12/2018 18:53   \n",
       "1                        Cloudesley Road, Angel  27/12/2018 19:45   \n",
       "2                      Risinghill Street, Angel  27/12/2018 20:09   \n",
       "3            Royal London Hospital, Whitechapel  28/12/2018 07:24   \n",
       "4  Aquatic Centre, Queen Elizabeth Olympic Park  26/12/2018 11:32   \n",
       "\n",
       "   StartStation Id                  StartStation Name  \n",
       "0               94          Bricklayers Arms, Borough  \n",
       "1              339           Risinghill Street, Angel  \n",
       "2              234  Liverpool Road (N1 Centre), Angel  \n",
       "3              698       Shoreditch Court, Haggerston  \n",
       "4              783          Monier Road, Hackney Wick  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data_2019.shape)\n",
    "data_2019.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rental Id</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Bike Id</th>\n",
       "      <th>End Date</th>\n",
       "      <th>EndStation Id</th>\n",
       "      <th>EndStation Name</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>StartStation Id</th>\n",
       "      <th>StartStation Name</th>\n",
       "      <th>Start Date Time</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83252102</td>\n",
       "      <td>720</td>\n",
       "      <td>2077</td>\n",
       "      <td>31/12/2018 19:05</td>\n",
       "      <td>272</td>\n",
       "      <td>Baylis Road, Waterloo</td>\n",
       "      <td>31/12/2018 18:53</td>\n",
       "      <td>94</td>\n",
       "      <td>Bricklayers Arms, Borough</td>\n",
       "      <td>2018-12-31 18:53:00</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83195883</td>\n",
       "      <td>120</td>\n",
       "      <td>10781</td>\n",
       "      <td>27/12/2018 19:47</td>\n",
       "      <td>93</td>\n",
       "      <td>Cloudesley Road, Angel</td>\n",
       "      <td>27/12/2018 19:45</td>\n",
       "      <td>339</td>\n",
       "      <td>Risinghill Street, Angel</td>\n",
       "      <td>2018-12-27 19:45:00</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83196070</td>\n",
       "      <td>120</td>\n",
       "      <td>2977</td>\n",
       "      <td>27/12/2018 20:11</td>\n",
       "      <td>339</td>\n",
       "      <td>Risinghill Street, Angel</td>\n",
       "      <td>27/12/2018 20:09</td>\n",
       "      <td>234</td>\n",
       "      <td>Liverpool Road (N1 Centre), Angel</td>\n",
       "      <td>2018-12-27 20:09:00</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83197932</td>\n",
       "      <td>660</td>\n",
       "      <td>10802</td>\n",
       "      <td>28/12/2018 07:35</td>\n",
       "      <td>282</td>\n",
       "      <td>Royal London Hospital, Whitechapel</td>\n",
       "      <td>28/12/2018 07:24</td>\n",
       "      <td>698</td>\n",
       "      <td>Shoreditch Court, Haggerston</td>\n",
       "      <td>2018-12-28 07:24:00</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83176351</td>\n",
       "      <td>1380</td>\n",
       "      <td>15749</td>\n",
       "      <td>26/12/2018 11:55</td>\n",
       "      <td>785</td>\n",
       "      <td>Aquatic Centre, Queen Elizabeth Olympic Park</td>\n",
       "      <td>26/12/2018 11:32</td>\n",
       "      <td>783</td>\n",
       "      <td>Monier Road, Hackney Wick</td>\n",
       "      <td>2018-12-26 11:32:00</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rental Id  Duration  Bike Id          End Date  EndStation Id  \\\n",
       "0   83252102       720     2077  31/12/2018 19:05            272   \n",
       "1   83195883       120    10781  27/12/2018 19:47             93   \n",
       "2   83196070       120     2977  27/12/2018 20:11            339   \n",
       "3   83197932       660    10802  28/12/2018 07:35            282   \n",
       "4   83176351      1380    15749  26/12/2018 11:55            785   \n",
       "\n",
       "                                EndStation Name        Start Date  \\\n",
       "0                         Baylis Road, Waterloo  31/12/2018 18:53   \n",
       "1                        Cloudesley Road, Angel  27/12/2018 19:45   \n",
       "2                      Risinghill Street, Angel  27/12/2018 20:09   \n",
       "3            Royal London Hospital, Whitechapel  28/12/2018 07:24   \n",
       "4  Aquatic Centre, Queen Elizabeth Olympic Park  26/12/2018 11:32   \n",
       "\n",
       "   StartStation Id                  StartStation Name     Start Date Time  \\\n",
       "0               94          Bricklayers Arms, Borough 2018-12-31 18:53:00   \n",
       "1              339           Risinghill Street, Angel 2018-12-27 19:45:00   \n",
       "2              234  Liverpool Road (N1 Centre), Angel 2018-12-27 20:09:00   \n",
       "3              698       Shoreditch Court, Haggerston 2018-12-28 07:24:00   \n",
       "4              783          Monier Road, Hackney Wick 2018-12-26 11:32:00   \n",
       "\n",
       "   Hour  Day  \n",
       "0    18    0  \n",
       "1    19    3  \n",
       "2    20    3  \n",
       "3     7    4  \n",
       "4    11    2  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2019\n",
    "\n",
    "## Add some extra variables to the dataset for use later in filtering\n",
    "\n",
    "import datetime\n",
    "\n",
    "## Feeding a specififed date format speeds up the pd.to_datetime function immeasurably, especially over large datasets\n",
    "## e.g. http://stackoverflow.com/questions/32034689/why-is-pandas-to-datetime-slow-for-non-standard-time-format-such-as-2014-12-31\n",
    "\n",
    "format = \"%d/%m/%Y %H:%M\"\n",
    "\n",
    "## Some routes had dates with a seconds component, whereas some didn't - the below code cuts these seconds off\n",
    "data_2019['Start Date']= data_2019['Start Date'].str[:16]\n",
    "\n",
    "data_2019['Start Date Time']= pd.to_datetime(data_2019['Start Date'], format=format)\n",
    "\n",
    "data_2019['Hour']= pd.to_datetime(data_2019['Start Date'], format=format).dt.hour\n",
    "\n",
    "data_2019['Day']= pd.to_datetime(data_2019['Start Date'], format=format).dt.weekday\n",
    "\n",
    "data_2019.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10310063, 12)\n"
     ]
    }
   ],
   "source": [
    "# 2019 filtering data - remove any rows that aren't from 2019\n",
    "# remember the first csv contained data from 2018... 26Dec2018-01Jan2019.csv\n",
    "bike_data_2019 = data_2019[data_2019['Start Date Time'].dt.year == 2019]\n",
    "print(bike_data_2019.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rental Id            0\n",
       "Duration             0\n",
       "Bike Id              0\n",
       "End Date             0\n",
       "EndStation Id        0\n",
       "EndStation Name      0\n",
       "Start Date           0\n",
       "StartStation Id      0\n",
       "StartStation Name    0\n",
       "Start Date Time      0\n",
       "Hour                 0\n",
       "Day                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bike_data_2019 has no null values, perfect\n",
    "#bike_data_2019.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2022 data prep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In September 2022 the column names change slightly and additional clumns have been added\n",
    "- for example the 'Bike model' column has been added (classic or PBSC_EBIKE)\n",
    "\n",
    "Cycle Hire Data - data format change & new data https://techforum.tfl.gov.uk/t/cycle-hire-data-data-format-change-new-data/2520"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the 2022 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_2022 = [item for item in all_csv if '2022' in item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSVs before September 2022 part 1 data \n",
    "# use slicing to includes all elements of the previous list except for the last 16\n",
    "csv_2022_p1 = csv_2022[:-16]\n",
    "\n",
    "# CSVs From september 12th 2022 \n",
    "# use slicing to create a new list that includes only the last 16 elements\n",
    "csv_2022_p2 = csv_2022[-16:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing the same for the 2022 data\n",
    "# passing errors within the csv files as per https://stackoverflow.com/questions/52105659/pandas-read-csv-unexpected-end-of-data-error\n",
    "dfs_2022_p1 = (pd.read_csv(csv, engine='python', encoding='utf-8', on_bad_lines='skip') for csv in csv_2022_p1)\n",
    "data_2022_p1 = pd.concat(dfs_2022_p1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rental Id                 0\n",
       "Duration                  0\n",
       "Bike Id                   0\n",
       "End Date                  0\n",
       "EndStation Id        312144\n",
       "EndStation Name           0\n",
       "Start Date                0\n",
       "StartStation Id           0\n",
       "StartStation Name         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2022_p1.isnull().sum()\n",
    "# for the part 1 data, there were 312144 records with null station ids  \n",
    "\n",
    "#es_id_null = data_2022_p1.loc[data_2022_p1['EndStation Id'].isnull()] \n",
    "#es_id_null.sort_values(by='Start Date', ascending=False)\n",
    "\n",
    "# filtering the data above reveal the journeys taken between 06/07/2022 00:00 and 12/07/2022 23:56 did not record an end station Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rental Id            8677104\n",
       "Duration             8677104\n",
       "Bike Id              8677104\n",
       "End Date             8677104\n",
       "EndStation Id        8364960\n",
       "EndStation Name      8677104\n",
       "Start Date           8677104\n",
       "StartStation Id      8677104\n",
       "StartStation Name    8677104\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2022_p1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EMoses\\AppData\\Local\\Temp\\ipykernel_2668\\3155316714.py:2: DtypeWarning: Columns (2,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs_2022_p2 = (pd.read_csv(csv) for csv in csv_2022_p2)\n"
     ]
    }
   ],
   "source": [
    "# read in data with datetime data type for column 2 and column 5\n",
    "dfs_2022_p2 = (pd.read_csv(csv) for csv in csv_2022_p2)\n",
    "#dfs_2022_p2 = (pd.read_csv(csv, parse_dates={'Start date': 'datetime64', 'End date': 'datetime64'}) for csv in csv_2022_p2)\n",
    "data_2022_p2 = pd.concat(dfs_2022_p2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Number                  0\n",
       "Start date              0\n",
       "Start station number    0\n",
       "Start station           0\n",
       "End date                0\n",
       "End station number      0\n",
       "End station             0\n",
       "Bike number             0\n",
       "Bike model              0\n",
       "Total duration          0\n",
       "Total duration (ms)     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2022_p2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Number                  2555077\n",
       "Start date              2555077\n",
       "Start station number    2555077\n",
       "Start station           2555077\n",
       "End date                2555077\n",
       "End station number      2555077\n",
       "End station             2555077\n",
       "Bike number             2555077\n",
       "Bike model              2555077\n",
       "Total duration          2555077\n",
       "Total duration (ms)     2555077\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2022_p2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing the same for the 2022 data\n",
    "# passing errors within the csv files as per https://stackoverflow.com/questions/52105659/pandas-read-csv-unexpected-end-of-data-error\n",
    "dfs_2022 = (pd.read_csv(csv, engine='python', encoding='utf-8', on_bad_lines='skip') for csv in csv_2022)\n",
    "data_2022 = pd.concat(dfs_2022, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n"
     ]
    }
   ],
   "source": [
    "# check the data type of the 'date' column\n",
    "print(data_2022['Start date'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022\n",
    "\n",
    "# Let's clean this up and get all the data into single columns\n",
    "\n",
    "\n",
    "#creating a copy of the orginal data\n",
    "data_2022_clean = data_2022.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's start by sorting out the date time formatting\n",
    "format = \"%d/%m/%Y %H:%M\"\n",
    "format2 = \"%Y/%m/%d %H:%M\"\n",
    "\n",
    "\n",
    "data_2022_clean['Start Date'] = data_2022_clean['Start Date'].str[:16]\n",
    "\n",
    "\n",
    "# let's create some extra columns to store the newly formatted datetime data,\n",
    "#remember the date columns have different formatting before and after September\n",
    "# we will merge them into single columns later on \n",
    "data_2022_clean['Start Date Time'] = pd.to_datetime(data_2022_clean['Start Date'], format=format)\n",
    "data_2022_clean['Start Date Time 2']= pd.to_datetime(data_2022_clean['Start date'], format=format2)\n",
    "data_2022_clean['End Date Time'] = pd.to_datetime(data_2022_clean['End Date'], format=format)\n",
    "data_2022_clean['End Date Time 2'] = pd.to_datetime(data_2022_clean['Start date'], format=format2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rental Id</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Bike Id</th>\n",
       "      <th>End Date</th>\n",
       "      <th>EndStation Id</th>\n",
       "      <th>EndStation Name</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>StartStation Id</th>\n",
       "      <th>StartStation Name</th>\n",
       "      <th>Number</th>\n",
       "      <th>...</th>\n",
       "      <th>End station number</th>\n",
       "      <th>End station</th>\n",
       "      <th>Bike number</th>\n",
       "      <th>Bike model</th>\n",
       "      <th>Total duration</th>\n",
       "      <th>Total duration (ms)</th>\n",
       "      <th>Start Date Time</th>\n",
       "      <th>Start Date Time 2</th>\n",
       "      <th>End Date Time</th>\n",
       "      <th>End Date Time 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115967515.0</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>15338.0</td>\n",
       "      <td>01/01/2022 23:13</td>\n",
       "      <td>310.0</td>\n",
       "      <td>Black Prince Road, Vauxhall</td>\n",
       "      <td>01/01/2022 22:52</td>\n",
       "      <td>529.0</td>\n",
       "      <td>Manresa Road, Chelsea</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-01-01 22:52:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-01-01 23:13:00</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116017034.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>19861.0</td>\n",
       "      <td>04/01/2022 19:08</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Brunswick Square, Bloomsbury</td>\n",
       "      <td>04/01/2022 18:56</td>\n",
       "      <td>804.0</td>\n",
       "      <td>Good's Way, King's Cross</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-01-04 18:56:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-01-04 19:08:00</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>115895660.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>19666.0</td>\n",
       "      <td>29/12/2021 16:34</td>\n",
       "      <td>70.0</td>\n",
       "      <td>Calshot Street , King's Cross</td>\n",
       "      <td>29/12/2021 16:28</td>\n",
       "      <td>57.0</td>\n",
       "      <td>Guilford Street , Bloomsbury</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-12-29 16:28:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2021-12-29 16:34:00</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>116016563.0</td>\n",
       "      <td>480.0</td>\n",
       "      <td>19861.0</td>\n",
       "      <td>04/01/2022 18:46</td>\n",
       "      <td>804.0</td>\n",
       "      <td>Good's Way, King's Cross</td>\n",
       "      <td>04/01/2022 18:38</td>\n",
       "      <td>57.0</td>\n",
       "      <td>Guilford Street , Bloomsbury</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-01-04 18:38:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-01-04 18:46:00</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>116014412.0</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>17235.0</td>\n",
       "      <td>04/01/2022 17:45</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Belgrove Street , King's Cross</td>\n",
       "      <td>04/01/2022 17:24</td>\n",
       "      <td>297.0</td>\n",
       "      <td>Geraldine Street, Elephant &amp; Castle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-01-04 17:24:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-01-04 17:45:00</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11232176</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>127641458.0</td>\n",
       "      <td>...</td>\n",
       "      <td>200249</td>\n",
       "      <td>Queen Mary's, Mile End</td>\n",
       "      <td>53664.0</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>1h 49m 4s</td>\n",
       "      <td>6544593.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-12-26 00:02:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-12-26 00:02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11232177</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>127641459.0</td>\n",
       "      <td>...</td>\n",
       "      <td>200147</td>\n",
       "      <td>Salmon Lane, Limehouse</td>\n",
       "      <td>54303.0</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>32m 16s</td>\n",
       "      <td>1936877.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-12-26 00:02:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-12-26 00:02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11232178</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>127641453.0</td>\n",
       "      <td>...</td>\n",
       "      <td>200160</td>\n",
       "      <td>Langdon Park, Poplar</td>\n",
       "      <td>21426.0</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>49m 15s</td>\n",
       "      <td>2955280.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-12-26 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-12-26 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11232179</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>127641454.0</td>\n",
       "      <td>...</td>\n",
       "      <td>22167</td>\n",
       "      <td>Millharbour, Millwall</td>\n",
       "      <td>54786.0</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>1h 30m 27s</td>\n",
       "      <td>5427555.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-12-26 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-12-26 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11232180</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>127641455.0</td>\n",
       "      <td>...</td>\n",
       "      <td>200160</td>\n",
       "      <td>Langdon Park, Poplar</td>\n",
       "      <td>30605.0</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>48m 22s</td>\n",
       "      <td>2902467.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-12-26 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-12-26 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11232181 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Rental Id  Duration  Bike Id          End Date  EndStation Id  \\\n",
       "0         115967515.0    1260.0  15338.0  01/01/2022 23:13          310.0   \n",
       "1         116017034.0     720.0  19861.0  04/01/2022 19:08           11.0   \n",
       "2         115895660.0     360.0  19666.0  29/12/2021 16:34           70.0   \n",
       "3         116016563.0     480.0  19861.0  04/01/2022 18:46          804.0   \n",
       "4         116014412.0    1260.0  17235.0  04/01/2022 17:45           14.0   \n",
       "...               ...       ...      ...               ...            ...   \n",
       "11232176          NaN       NaN      NaN               NaN            NaN   \n",
       "11232177          NaN       NaN      NaN               NaN            NaN   \n",
       "11232178          NaN       NaN      NaN               NaN            NaN   \n",
       "11232179          NaN       NaN      NaN               NaN            NaN   \n",
       "11232180          NaN       NaN      NaN               NaN            NaN   \n",
       "\n",
       "                         EndStation Name        Start Date  StartStation Id  \\\n",
       "0            Black Prince Road, Vauxhall  01/01/2022 22:52            529.0   \n",
       "1           Brunswick Square, Bloomsbury  04/01/2022 18:56            804.0   \n",
       "2          Calshot Street , King's Cross  29/12/2021 16:28             57.0   \n",
       "3               Good's Way, King's Cross  04/01/2022 18:38             57.0   \n",
       "4         Belgrove Street , King's Cross  04/01/2022 17:24            297.0   \n",
       "...                                  ...               ...              ...   \n",
       "11232176                             NaN               NaN              NaN   \n",
       "11232177                             NaN               NaN              NaN   \n",
       "11232178                             NaN               NaN              NaN   \n",
       "11232179                             NaN               NaN              NaN   \n",
       "11232180                             NaN               NaN              NaN   \n",
       "\n",
       "                            StartStation Name       Number  ...  \\\n",
       "0                       Manresa Road, Chelsea          NaN  ...   \n",
       "1                    Good's Way, King's Cross          NaN  ...   \n",
       "2                Guilford Street , Bloomsbury          NaN  ...   \n",
       "3                Guilford Street , Bloomsbury          NaN  ...   \n",
       "4         Geraldine Street, Elephant & Castle          NaN  ...   \n",
       "...                                       ...          ...  ...   \n",
       "11232176                                  NaN  127641458.0  ...   \n",
       "11232177                                  NaN  127641459.0  ...   \n",
       "11232178                                  NaN  127641453.0  ...   \n",
       "11232179                                  NaN  127641454.0  ...   \n",
       "11232180                                  NaN  127641455.0  ...   \n",
       "\n",
       "         End station number             End station Bike number Bike model  \\\n",
       "0                       NaN                     NaN         NaN        NaN   \n",
       "1                       NaN                     NaN         NaN        NaN   \n",
       "2                       NaN                     NaN         NaN        NaN   \n",
       "3                       NaN                     NaN         NaN        NaN   \n",
       "4                       NaN                     NaN         NaN        NaN   \n",
       "...                     ...                     ...         ...        ...   \n",
       "11232176             200249  Queen Mary's, Mile End     53664.0    CLASSIC   \n",
       "11232177             200147  Salmon Lane, Limehouse     54303.0    CLASSIC   \n",
       "11232178             200160    Langdon Park, Poplar     21426.0    CLASSIC   \n",
       "11232179              22167   Millharbour, Millwall     54786.0    CLASSIC   \n",
       "11232180             200160    Langdon Park, Poplar     30605.0    CLASSIC   \n",
       "\n",
       "         Total duration Total duration (ms)     Start Date Time  \\\n",
       "0                   NaN                 NaN 2022-01-01 22:52:00   \n",
       "1                   NaN                 NaN 2022-01-04 18:56:00   \n",
       "2                   NaN                 NaN 2021-12-29 16:28:00   \n",
       "3                   NaN                 NaN 2022-01-04 18:38:00   \n",
       "4                   NaN                 NaN 2022-01-04 17:24:00   \n",
       "...                 ...                 ...                 ...   \n",
       "11232176      1h 49m 4s           6544593.0                 NaT   \n",
       "11232177        32m 16s           1936877.0                 NaT   \n",
       "11232178        49m 15s           2955280.0                 NaT   \n",
       "11232179     1h 30m 27s           5427555.0                 NaT   \n",
       "11232180        48m 22s           2902467.0                 NaT   \n",
       "\n",
       "           Start Date Time 2       End Date Time     End Date Time 2  \n",
       "0                        NaT 2022-01-01 23:13:00                 NaT  \n",
       "1                        NaT 2022-01-04 19:08:00                 NaT  \n",
       "2                        NaT 2021-12-29 16:34:00                 NaT  \n",
       "3                        NaT 2022-01-04 18:46:00                 NaT  \n",
       "4                        NaT 2022-01-04 17:45:00                 NaT  \n",
       "...                      ...                 ...                 ...  \n",
       "11232176 2022-12-26 00:02:00                 NaT 2022-12-26 00:02:00  \n",
       "11232177 2022-12-26 00:02:00                 NaT 2022-12-26 00:02:00  \n",
       "11232178 2022-12-26 00:00:00                 NaT 2022-12-26 00:00:00  \n",
       "11232179 2022-12-26 00:00:00                 NaT 2022-12-26 00:00:00  \n",
       "11232180 2022-12-26 00:00:00                 NaT 2022-12-26 00:00:00  \n",
       "\n",
       "[11232181 rows x 24 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2022_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rental Id               2555077\n",
       "Duration                2555077\n",
       "Bike Id                 2555077\n",
       "End Date                2555077\n",
       "EndStation Id           2867221\n",
       "EndStation Name         2555077\n",
       "Start Date              2555077\n",
       "StartStation Id         2555077\n",
       "StartStation Name       2555077\n",
       "Number                  8677104\n",
       "Start date              8677104\n",
       "Start station number    8677104\n",
       "Start station           8677104\n",
       "End date                8677104\n",
       "End station number      8677104\n",
       "End station             8677104\n",
       "Bike number             8677104\n",
       "Bike model              8677104\n",
       "Total duration          8677104\n",
       "Total duration (ms)     8677104\n",
       "Start Date Time         2555077\n",
       "Start Date Time 2       8677104\n",
       "End Date Time           2555077\n",
       "End Date Time 2         8677104\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2022_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfering values from one pandas column to another pandas column only for null rows\n",
    "\n",
    "data_2022_clean.loc[data_2022_clean['Rental Id'].isnull(), 'Rental Id'] = data_2022_clean['Number']\n",
    "# converting from milliseconds to seconds, multipyling by 1000 \n",
    "data_2022_clean.loc[data_2022_clean['Duration'].isnull(), 'Duration'] = data_2022_clean['Total duration (ms)'] / 1000\n",
    "data_2022_clean.loc[data_2022_clean['Bike Id'].isnull(), 'Bike Id'] = data_2022_clean['Bike number']\n",
    "data_2022_clean.loc[data_2022_clean['End Date'].isnull(), 'End Date'] = data_2022_clean['End date']\n",
    "data_2022_clean.loc[data_2022_clean['EndStation Name'].isnull(), 'EndStation Name'] = data_2022_clean['End station']\n",
    "data_2022_clean.loc[data_2022_clean['Start Date'].isnull(), 'Start Date'] = data_2022_clean['Start Date Time 2']\n",
    "data_2022_clean.loc[data_2022_clean['End Date'].isnull(), 'End Date'] = data_2022_clean['End Date Time']\n",
    "data_2022_clean.loc[data_2022_clean['StartStation Name'].isnull(), 'StartStation Name'] = data_2022_clean['Start station']\n",
    "\n",
    "#data_2022_clean.sort_values(by='Bike model', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rental Id                     0\n",
       "Duration                      0\n",
       "Bike Id                       0\n",
       "End Date                      0\n",
       "EndStation Id           2867221\n",
       "EndStation Name               0\n",
       "Start Date                    0\n",
       "StartStation Id         2555077\n",
       "StartStation Name             0\n",
       "Number                  8677104\n",
       "Start date              8677104\n",
       "Start station number    8677104\n",
       "Start station           8677104\n",
       "End date                8677104\n",
       "End station number      8677104\n",
       "End station             8677104\n",
       "Bike number             8677104\n",
       "Bike model              8677104\n",
       "Total duration          8677104\n",
       "Total duration (ms)     8677104\n",
       "Start Date Time         2555077\n",
       "Start Date Time 2       8677104\n",
       "End Date Time           2555077\n",
       "End Date Time 2         8677104\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2022_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensuring a consistent format of the date columns, initially in string format\n",
    "data_2022_clean['Start Date'] = pd.to_datetime(data_2022_clean['Start Date']).dt.strftime('%d/%m/%Y %H:%M')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing the same for end date\n",
    "data_2022_clean['End Date'] = pd.to_datetime(data_2022_clean['End Date']).dt.strftime('%d/%m/%Y %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_2022_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data_2022_clean\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_2022_clean' is not defined"
     ]
    }
   ],
   "source": [
    "data_2022_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 686. MiB for an array with shape (8, 11232181) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data_2022_clean1 \u001b[39m=\u001b[39m data_2022_clean\u001b[39m.\u001b[39;49mcopy()\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\core\\generic.py:6368\u001b[0m, in \u001b[0;36mNDFrame.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m   6258\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m   6259\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcopy\u001b[39m(\u001b[39mself\u001b[39m: NDFrameT, deep: bool_t \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NDFrameT:\n\u001b[0;32m   6260\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   6261\u001b[0m \u001b[39m    Make a copy of this object's indices and data.\u001b[39;00m\n\u001b[0;32m   6262\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6366\u001b[0m \u001b[39m    dtype: object\u001b[39;00m\n\u001b[0;32m   6367\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 6368\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mcopy(deep\u001b[39m=\u001b[39;49mdeep)\n\u001b[0;32m   6369\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clear_item_cache()\n\u001b[0;32m   6370\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(data)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcopy\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\core\\internals\\managers.py:649\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    647\u001b[0m     new_axes \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes)\n\u001b[1;32m--> 649\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(\u001b[39m\"\u001b[39;49m\u001b[39mcopy\u001b[39;49m\u001b[39m\"\u001b[39;49m, deep\u001b[39m=\u001b[39;49mdeep)\n\u001b[0;32m    650\u001b[0m new_refs: \u001b[39mlist\u001b[39m[weakref\u001b[39m.\u001b[39mref \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    651\u001b[0m \u001b[39mif\u001b[39;00m deep:\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\core\\internals\\managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m         applied \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39mapply(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    351\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 352\u001b[0m         applied \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(b, f)(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mNotImplementedError\u001b[39;00m):\n\u001b[0;32m    354\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ignore_failures:\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:549\u001b[0m, in \u001b[0;36mBlock.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    547\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues\n\u001b[0;32m    548\u001b[0m \u001b[39mif\u001b[39;00m deep:\n\u001b[1;32m--> 549\u001b[0m     values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39;49mcopy()\n\u001b[0;32m    550\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)(values, placement\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr_locs, ndim\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 686. MiB for an array with shape (8, 11232181) and data type float64"
     ]
    }
   ],
   "source": [
    "data_2022_clean1 = data_2022_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding hour and day columns  \n",
    "data_2022_clean1['Hour']= pd.to_datetime(data_2022_clean1['Start Date'], format = \"%d/%m/%Y %H:%M\").dt.hour\n",
    "data_2022_clean1['Day']= pd.to_datetime(data_2022_clean1['Start Date'], format = \"%d/%m/%Y %H:%M\").dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 428. MiB for an array with shape (5, 11232181) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# removing columns that are no longer needed\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data_2022_clean_drop \u001b[39m=\u001b[39m data_2022_clean1\u001b[39m.\u001b[39;49mdrop([\u001b[39m'\u001b[39;49m\u001b[39mNumber\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mStart date\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mStart station\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mEnd date\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mEnd station\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      3\u001b[0m                                              \u001b[39m'\u001b[39;49m\u001b[39mBike number\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mTotal duration\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mTotal duration (ms)\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mStart Date Time\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mStart Date Time 2\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mEnd Date Time\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mEnd Date Time 2\u001b[39;49m\u001b[39m'\u001b[39;49m], axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\core\\frame.py:5399\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5251\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m   5252\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m   5253\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5260\u001b[0m     errors: IgnoreRaise \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   5261\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   5262\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   5263\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5264\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5397\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5398\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5399\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[0;32m   5400\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   5401\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   5402\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   5403\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   5404\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   5405\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[0;32m   5406\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   5407\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\core\\generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4503\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   4504\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 4505\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4507\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[0;32m   4508\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\core\\generic.py:4585\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4582\u001b[0m     new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m   4584\u001b[0m bm_axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim \u001b[39m-\u001b[39m axis_num \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 4585\u001b[0m new_mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mreindex_indexer(\n\u001b[0;32m   4586\u001b[0m     new_axis,\n\u001b[0;32m   4587\u001b[0m     indexer,\n\u001b[0;32m   4588\u001b[0m     axis\u001b[39m=\u001b[39;49mbm_axis,\n\u001b[0;32m   4589\u001b[0m     allow_dups\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   4590\u001b[0m     only_slice\u001b[39m=\u001b[39;49monly_slice,\n\u001b[0;32m   4591\u001b[0m )\n\u001b[0;32m   4592\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_mgr)\n\u001b[0;32m   4593\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\core\\internals\\managers.py:743\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    740\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mRequested axis not found in manager\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    742\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 743\u001b[0m     new_blocks, new_refs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_slice_take_blocks_ax0(\n\u001b[0;32m    744\u001b[0m         indexer,\n\u001b[0;32m    745\u001b[0m         fill_value\u001b[39m=\u001b[39;49mfill_value,\n\u001b[0;32m    746\u001b[0m         only_slice\u001b[39m=\u001b[39;49monly_slice,\n\u001b[0;32m    747\u001b[0m         use_na_proxy\u001b[39m=\u001b[39;49muse_na_proxy,\n\u001b[0;32m    748\u001b[0m     )\n\u001b[0;32m    749\u001b[0m     parent \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mall_none(\u001b[39m*\u001b[39mnew_refs) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[0;32m    750\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\core\\internals\\managers.py:912\u001b[0m, in \u001b[0;36mBaseBlockManager._slice_take_blocks_ax0\u001b[1;34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    910\u001b[0m         refs\u001b[39m.\u001b[39mappend(weakref\u001b[39m.\u001b[39mref(blk))\n\u001b[0;32m    911\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 912\u001b[0m     nb \u001b[39m=\u001b[39m blk\u001b[39m.\u001b[39;49mtake_nd(taker, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, new_mgr_locs\u001b[39m=\u001b[39;49mmgr_locs)\n\u001b[0;32m    913\u001b[0m     blocks\u001b[39m.\u001b[39mappend(nb)\n\u001b[0;32m    914\u001b[0m     refs\u001b[39m.\u001b[39mappend(\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:880\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m    877\u001b[0m     allow_fill \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[39m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[1;32m--> 880\u001b[0m new_values \u001b[39m=\u001b[39m algos\u001b[39m.\u001b[39;49mtake_nd(\n\u001b[0;32m    881\u001b[0m     values, indexer, axis\u001b[39m=\u001b[39;49maxis, allow_fill\u001b[39m=\u001b[39;49mallow_fill, fill_value\u001b[39m=\u001b[39;49mfill_value\n\u001b[0;32m    882\u001b[0m )\n\u001b[0;32m    884\u001b[0m \u001b[39m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[0;32m    885\u001b[0m \u001b[39m#  this assertion\u001b[39;00m\n\u001b[0;32m    886\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m (axis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m new_mgr_locs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mtake(indexer, fill_value\u001b[39m=\u001b[39mfill_value, allow_fill\u001b[39m=\u001b[39mallow_fill)\n\u001b[0;32m    116\u001b[0m arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(arr)\n\u001b[1;32m--> 117\u001b[0m \u001b[39mreturn\u001b[39;00m _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\core\\array_algos\\take.py:158\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    156\u001b[0m     out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(out_shape, dtype\u001b[39m=\u001b[39mdtype, order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    157\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mempty(out_shape, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    160\u001b[0m func \u001b[39m=\u001b[39m _get_take_nd_function(\n\u001b[0;32m    161\u001b[0m     arr\u001b[39m.\u001b[39mndim, arr\u001b[39m.\u001b[39mdtype, out\u001b[39m.\u001b[39mdtype, axis\u001b[39m=\u001b[39maxis, mask_info\u001b[39m=\u001b[39mmask_info\n\u001b[0;32m    162\u001b[0m )\n\u001b[0;32m    163\u001b[0m func(arr, indexer, out, fill_value)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 428. MiB for an array with shape (5, 11232181) and data type float64"
     ]
    }
   ],
   "source": [
    "# removing columns that are no longer needed\n",
    "data_2022_clean_drop = data_2022_clean1.drop(['Number', 'Start date', 'Start station', 'End date', 'End station',\n",
    "                                             'Bike number', 'Total duration', 'Total duration (ms)', 'Start Date Time', 'Start Date Time 2','End Date Time','End Date Time 2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rental Id</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Bike Id</th>\n",
       "      <th>End Date</th>\n",
       "      <th>EndStation Id</th>\n",
       "      <th>EndStation Name</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>StartStation Id</th>\n",
       "      <th>StartStation Name</th>\n",
       "      <th>Start station number</th>\n",
       "      <th>End station number</th>\n",
       "      <th>Bike model</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115967515.0</td>\n",
       "      <td>1260.000</td>\n",
       "      <td>15338.0</td>\n",
       "      <td>01/01/2022 23:13</td>\n",
       "      <td>310.0</td>\n",
       "      <td>Black Prince Road, Vauxhall</td>\n",
       "      <td>01/01/2022 22:52</td>\n",
       "      <td>529.0</td>\n",
       "      <td>Manresa Road, Chelsea</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116017034.0</td>\n",
       "      <td>720.000</td>\n",
       "      <td>19861.0</td>\n",
       "      <td>01/04/2022 19:08</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Brunswick Square, Bloomsbury</td>\n",
       "      <td>01/04/2022 18:56</td>\n",
       "      <td>804.0</td>\n",
       "      <td>Good's Way, King's Cross</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>115895660.0</td>\n",
       "      <td>360.000</td>\n",
       "      <td>19666.0</td>\n",
       "      <td>29/12/2021 16:34</td>\n",
       "      <td>70.0</td>\n",
       "      <td>Calshot Street , King's Cross</td>\n",
       "      <td>29/12/2021 16:28</td>\n",
       "      <td>57.0</td>\n",
       "      <td>Guilford Street , Bloomsbury</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>116016563.0</td>\n",
       "      <td>480.000</td>\n",
       "      <td>19861.0</td>\n",
       "      <td>01/04/2022 18:46</td>\n",
       "      <td>804.0</td>\n",
       "      <td>Good's Way, King's Cross</td>\n",
       "      <td>01/04/2022 18:38</td>\n",
       "      <td>57.0</td>\n",
       "      <td>Guilford Street , Bloomsbury</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>116014412.0</td>\n",
       "      <td>1260.000</td>\n",
       "      <td>17235.0</td>\n",
       "      <td>01/04/2022 17:45</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Belgrove Street , King's Cross</td>\n",
       "      <td>01/04/2022 17:24</td>\n",
       "      <td>297.0</td>\n",
       "      <td>Geraldine Street, Elephant &amp; Castle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11232176</th>\n",
       "      <td>127641458.0</td>\n",
       "      <td>6544.593</td>\n",
       "      <td>53664.0</td>\n",
       "      <td>26/12/2022 01:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Queen Mary's, Mile End</td>\n",
       "      <td>26/12/2022 00:02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Woodstock Grove, Shepherd's Bush</td>\n",
       "      <td>200214</td>\n",
       "      <td>200249</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11232177</th>\n",
       "      <td>127641459.0</td>\n",
       "      <td>1936.877</td>\n",
       "      <td>54303.0</td>\n",
       "      <td>26/12/2022 00:34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Salmon Lane, Limehouse</td>\n",
       "      <td>26/12/2022 00:02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Curlew Street, Shad Thames</td>\n",
       "      <td>1213</td>\n",
       "      <td>200147</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11232178</th>\n",
       "      <td>127641453.0</td>\n",
       "      <td>2955.280</td>\n",
       "      <td>21426.0</td>\n",
       "      <td>26/12/2022 00:49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Langdon Park, Poplar</td>\n",
       "      <td>26/12/2022 00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Curlew Street, Shad Thames</td>\n",
       "      <td>1213</td>\n",
       "      <td>200160</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11232179</th>\n",
       "      <td>127641454.0</td>\n",
       "      <td>5427.555</td>\n",
       "      <td>54786.0</td>\n",
       "      <td>26/12/2022 01:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Millharbour, Millwall</td>\n",
       "      <td>26/12/2022 00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Millharbour, Millwall</td>\n",
       "      <td>22167</td>\n",
       "      <td>22167</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11232180</th>\n",
       "      <td>127641455.0</td>\n",
       "      <td>2902.467</td>\n",
       "      <td>30605.0</td>\n",
       "      <td>26/12/2022 00:49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Langdon Park, Poplar</td>\n",
       "      <td>26/12/2022 00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Curlew Street, Shad Thames</td>\n",
       "      <td>1213</td>\n",
       "      <td>200160</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11232181 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Rental Id  Duration  Bike Id          End Date  EndStation Id  \\\n",
       "0         115967515.0  1260.000  15338.0  01/01/2022 23:13          310.0   \n",
       "1         116017034.0   720.000  19861.0  01/04/2022 19:08           11.0   \n",
       "2         115895660.0   360.000  19666.0  29/12/2021 16:34           70.0   \n",
       "3         116016563.0   480.000  19861.0  01/04/2022 18:46          804.0   \n",
       "4         116014412.0  1260.000  17235.0  01/04/2022 17:45           14.0   \n",
       "...               ...       ...      ...               ...            ...   \n",
       "11232176  127641458.0  6544.593  53664.0  26/12/2022 01:51            NaN   \n",
       "11232177  127641459.0  1936.877  54303.0  26/12/2022 00:34            NaN   \n",
       "11232178  127641453.0  2955.280  21426.0  26/12/2022 00:49            NaN   \n",
       "11232179  127641454.0  5427.555  54786.0  26/12/2022 01:31            NaN   \n",
       "11232180  127641455.0  2902.467  30605.0  26/12/2022 00:49            NaN   \n",
       "\n",
       "                         EndStation Name        Start Date  StartStation Id  \\\n",
       "0            Black Prince Road, Vauxhall  01/01/2022 22:52            529.0   \n",
       "1           Brunswick Square, Bloomsbury  01/04/2022 18:56            804.0   \n",
       "2          Calshot Street , King's Cross  29/12/2021 16:28             57.0   \n",
       "3               Good's Way, King's Cross  01/04/2022 18:38             57.0   \n",
       "4         Belgrove Street , King's Cross  01/04/2022 17:24            297.0   \n",
       "...                                  ...               ...              ...   \n",
       "11232176          Queen Mary's, Mile End  26/12/2022 00:02              NaN   \n",
       "11232177          Salmon Lane, Limehouse  26/12/2022 00:02              NaN   \n",
       "11232178            Langdon Park, Poplar  26/12/2022 00:00              NaN   \n",
       "11232179           Millharbour, Millwall  26/12/2022 00:00              NaN   \n",
       "11232180            Langdon Park, Poplar  26/12/2022 00:00              NaN   \n",
       "\n",
       "                            StartStation Name Start station number  \\\n",
       "0                       Manresa Road, Chelsea                  NaN   \n",
       "1                    Good's Way, King's Cross                  NaN   \n",
       "2                Guilford Street , Bloomsbury                  NaN   \n",
       "3                Guilford Street , Bloomsbury                  NaN   \n",
       "4         Geraldine Street, Elephant & Castle                  NaN   \n",
       "...                                       ...                  ...   \n",
       "11232176     Woodstock Grove, Shepherd's Bush               200214   \n",
       "11232177           Curlew Street, Shad Thames                 1213   \n",
       "11232178           Curlew Street, Shad Thames                 1213   \n",
       "11232179                Millharbour, Millwall                22167   \n",
       "11232180           Curlew Street, Shad Thames                 1213   \n",
       "\n",
       "         End station number Bike model  Hour  Day  \n",
       "0                       NaN        NaN    22    5  \n",
       "1                       NaN        NaN    18    4  \n",
       "2                       NaN        NaN    16    2  \n",
       "3                       NaN        NaN    18    4  \n",
       "4                       NaN        NaN    17    4  \n",
       "...                     ...        ...   ...  ...  \n",
       "11232176             200249    CLASSIC     0    0  \n",
       "11232177             200147    CLASSIC     0    0  \n",
       "11232178             200160    CLASSIC     0    0  \n",
       "11232179              22167    CLASSIC     0    0  \n",
       "11232180             200160    CLASSIC     0    0  \n",
       "\n",
       "[11232181 rows x 14 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2022_clean_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rental Id                     0\n",
       "Duration                      0\n",
       "Bike Id                       0\n",
       "End Date                      0\n",
       "EndStation Id           2867221\n",
       "EndStation Name               0\n",
       "Start Date                    0\n",
       "StartStation Id         2555077\n",
       "StartStation Name             0\n",
       "Start station number    8677104\n",
       "End station number      8677104\n",
       "Bike model              8677104\n",
       "Hour                          0\n",
       "Day                           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2022_clean_drop.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's rename a couple of columns to make it clearer\n",
    "# we will rename the Start and End station number column \n",
    "# these columns actually terminal to the station 'terminalName' as per https://tfl.gov.uk/tfl/syndication/feeds/cycle-hire/livecyclehireupdates.xml\n",
    "\n",
    "data_2022_clean_drop = data_2022_clean_drop.rename(columns={'Start station number': 'SS Terminal Name', 'End station number': 'ES Terminal Name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 85.2 MiB for an array with shape (11166111, 1) and data type datetime64[ns]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# finally, 2022 filtering data - remove any rows that aren't from 2022\u001b[39;00m\n\u001b[0;32m      2\u001b[0m data_2022_clean_drop[\u001b[39m'\u001b[39m\u001b[39mStart Date Time\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(data_2022_clean_drop[\u001b[39m\"\u001b[39m\u001b[39mStart Date\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm/\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY \u001b[39m\u001b[39m%\u001b[39m\u001b[39mH:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m data_2022_clean_drop1 \u001b[39m=\u001b[39m data_2022_clean_drop[data_2022_clean_drop[\u001b[39m'\u001b[39;49m\u001b[39mStart Date Time\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mdt\u001b[39m.\u001b[39;49myear \u001b[39m==\u001b[39;49m \u001b[39m2022\u001b[39;49m]\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(data_2022_clean_drop1\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\core\\frame.py:3798\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3796\u001b[0m \u001b[39m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[0;32m   3797\u001b[0m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mis_bool_indexer(key):\n\u001b[1;32m-> 3798\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_bool_array(key)\n\u001b[0;32m   3800\u001b[0m \u001b[39m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m \u001b[39m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m is_single_key \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(key, \u001b[39mtuple\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m is_list_like(key)\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\core\\frame.py:3853\u001b[0m, in \u001b[0;36mDataFrame._getitem_bool_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3851\u001b[0m key \u001b[39m=\u001b[39m check_bool_indexer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, key)\n\u001b[0;32m   3852\u001b[0m indexer \u001b[39m=\u001b[39m key\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]\n\u001b[1;32m-> 3853\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_with_is_copy(indexer, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\core\\generic.py:3902\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   3894\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_take_with_is_copy\u001b[39m(\u001b[39mself\u001b[39m: NDFrameT, indices, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NDFrameT:\n\u001b[0;32m   3895\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3896\u001b[0m \u001b[39m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[0;32m   3897\u001b[0m \u001b[39m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3900\u001b[0m \u001b[39m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   3901\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3902\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take(indices\u001b[39m=\u001b[39;49mindices, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   3903\u001b[0m     \u001b[39m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[0;32m   3904\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m result\u001b[39m.\u001b[39m_get_axis(axis)\u001b[39m.\u001b[39mequals(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\core\\generic.py:3886\u001b[0m, in \u001b[0;36mNDFrame._take\u001b[1;34m(self, indices, axis, convert_indices)\u001b[0m\n\u001b[0;32m   3879\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3880\u001b[0m \u001b[39mInternal version of the `take` allowing specification of additional args.\u001b[39;00m\n\u001b[0;32m   3881\u001b[0m \n\u001b[0;32m   3882\u001b[0m \u001b[39mSee the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   3883\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3884\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_consolidate_inplace()\n\u001b[1;32m-> 3886\u001b[0m new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mtake(\n\u001b[0;32m   3887\u001b[0m     indices,\n\u001b[0;32m   3888\u001b[0m     axis\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_block_manager_axis(axis),\n\u001b[0;32m   3889\u001b[0m     verify\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   3890\u001b[0m     convert_indices\u001b[39m=\u001b[39;49mconvert_indices,\n\u001b[0;32m   3891\u001b[0m )\n\u001b[0;32m   3892\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_data)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtake\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\core\\internals\\managers.py:978\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify, convert_indices)\u001b[0m\n\u001b[0;32m    975\u001b[0m     indexer \u001b[39m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[39m=\u001b[39mverify)\n\u001b[0;32m    977\u001b[0m new_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes[axis]\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m--> 978\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreindex_indexer(\n\u001b[0;32m    979\u001b[0m     new_axis\u001b[39m=\u001b[39;49mnew_labels,\n\u001b[0;32m    980\u001b[0m     indexer\u001b[39m=\u001b[39;49mindexer,\n\u001b[0;32m    981\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m    982\u001b[0m     allow_dups\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    983\u001b[0m     copy\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    984\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\core\\internals\\managers.py:751\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    749\u001b[0m     parent \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mall_none(\u001b[39m*\u001b[39mnew_refs) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[0;32m    750\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 751\u001b[0m     new_blocks \u001b[39m=\u001b[39m [\n\u001b[0;32m    752\u001b[0m         blk\u001b[39m.\u001b[39mtake_nd(\n\u001b[0;32m    753\u001b[0m             indexer,\n\u001b[0;32m    754\u001b[0m             axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m    755\u001b[0m             fill_value\u001b[39m=\u001b[39m(\n\u001b[0;32m    756\u001b[0m                 fill_value \u001b[39mif\u001b[39;00m fill_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m blk\u001b[39m.\u001b[39mfill_value\n\u001b[0;32m    757\u001b[0m             ),\n\u001b[0;32m    758\u001b[0m         )\n\u001b[0;32m    759\u001b[0m         \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks\n\u001b[0;32m    760\u001b[0m     ]\n\u001b[0;32m    761\u001b[0m     new_refs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    762\u001b[0m     parent \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\core\\internals\\managers.py:752\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    749\u001b[0m     parent \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mall_none(\u001b[39m*\u001b[39mnew_refs) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[0;32m    750\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    751\u001b[0m     new_blocks \u001b[39m=\u001b[39m [\n\u001b[1;32m--> 752\u001b[0m         blk\u001b[39m.\u001b[39;49mtake_nd(\n\u001b[0;32m    753\u001b[0m             indexer,\n\u001b[0;32m    754\u001b[0m             axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m    755\u001b[0m             fill_value\u001b[39m=\u001b[39;49m(\n\u001b[0;32m    756\u001b[0m                 fill_value \u001b[39mif\u001b[39;49;00m fill_value \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m blk\u001b[39m.\u001b[39;49mfill_value\n\u001b[0;32m    757\u001b[0m             ),\n\u001b[0;32m    758\u001b[0m         )\n\u001b[0;32m    759\u001b[0m         \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks\n\u001b[0;32m    760\u001b[0m     ]\n\u001b[0;32m    761\u001b[0m     new_refs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    762\u001b[0m     parent \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:880\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m    877\u001b[0m     allow_fill \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[39m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[1;32m--> 880\u001b[0m new_values \u001b[39m=\u001b[39m algos\u001b[39m.\u001b[39;49mtake_nd(\n\u001b[0;32m    881\u001b[0m     values, indexer, axis\u001b[39m=\u001b[39;49maxis, allow_fill\u001b[39m=\u001b[39;49mallow_fill, fill_value\u001b[39m=\u001b[39;49mfill_value\n\u001b[0;32m    882\u001b[0m )\n\u001b[0;32m    884\u001b[0m \u001b[39m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[0;32m    885\u001b[0m \u001b[39m#  this assertion\u001b[39;00m\n\u001b[0;32m    886\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m (axis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m new_mgr_locs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\core\\array_algos\\take.py:110\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_1d_only_ea_obj(arr):\n\u001b[0;32m    108\u001b[0m         \u001b[39m# i.e. DatetimeArray, TimedeltaArray\u001b[39;00m\n\u001b[0;32m    109\u001b[0m         arr \u001b[39m=\u001b[39m cast(\u001b[39m\"\u001b[39m\u001b[39mNDArrayBackedExtensionArray\u001b[39m\u001b[39m\"\u001b[39m, arr)\n\u001b[1;32m--> 110\u001b[0m         \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39;49mtake(\n\u001b[0;32m    111\u001b[0m             indexer, fill_value\u001b[39m=\u001b[39;49mfill_value, allow_fill\u001b[39m=\u001b[39;49mallow_fill, axis\u001b[39m=\u001b[39;49maxis\n\u001b[0;32m    112\u001b[0m         )\n\u001b[0;32m    114\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mtake(indexer, fill_value\u001b[39m=\u001b[39mfill_value, allow_fill\u001b[39m=\u001b[39mallow_fill)\n\u001b[0;32m    116\u001b[0m arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(arr)\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\core\\arrays\\_mixins.py:165\u001b[0m, in \u001b[0;36mNDArrayBackedExtensionArray.take\u001b[1;34m(self, indices, allow_fill, fill_value, axis)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[39mif\u001b[39;00m allow_fill:\n\u001b[0;32m    163\u001b[0m     fill_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_scalar(fill_value)\n\u001b[1;32m--> 165\u001b[0m new_data \u001b[39m=\u001b[39m take(\n\u001b[0;32m    166\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ndarray,\n\u001b[0;32m    167\u001b[0m     indices,\n\u001b[0;32m    168\u001b[0m     allow_fill\u001b[39m=\u001b[39;49mallow_fill,\n\u001b[0;32m    169\u001b[0m     fill_value\u001b[39m=\u001b[39;49mfill_value,\n\u001b[0;32m    170\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m    171\u001b[0m )\n\u001b[0;32m    172\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_from_backing_data(new_data)\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\core\\algorithms.py:1572\u001b[0m, in \u001b[0;36mtake\u001b[1;34m(arr, indices, axis, allow_fill, fill_value)\u001b[0m\n\u001b[0;32m   1569\u001b[0m \u001b[39mif\u001b[39;00m allow_fill:\n\u001b[0;32m   1570\u001b[0m     \u001b[39m# Pandas style, -1 means NA\u001b[39;00m\n\u001b[0;32m   1571\u001b[0m     validate_indices(indices, arr\u001b[39m.\u001b[39mshape[axis])\n\u001b[1;32m-> 1572\u001b[0m     result \u001b[39m=\u001b[39m take_nd(\n\u001b[0;32m   1573\u001b[0m         arr, indices, axis\u001b[39m=\u001b[39;49maxis, allow_fill\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, fill_value\u001b[39m=\u001b[39;49mfill_value\n\u001b[0;32m   1574\u001b[0m     )\n\u001b[0;32m   1575\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1576\u001b[0m     \u001b[39m# NumPy style\u001b[39;00m\n\u001b[0;32m   1577\u001b[0m     result \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mtake(indices, axis\u001b[39m=\u001b[39maxis)\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mtake(indexer, fill_value\u001b[39m=\u001b[39mfill_value, allow_fill\u001b[39m=\u001b[39mallow_fill)\n\u001b[0;32m    116\u001b[0m arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(arr)\n\u001b[1;32m--> 117\u001b[0m \u001b[39mreturn\u001b[39;00m _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\core\\array_algos\\take.py:158\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    156\u001b[0m     out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(out_shape, dtype\u001b[39m=\u001b[39mdtype, order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    157\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mempty(out_shape, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    160\u001b[0m func \u001b[39m=\u001b[39m _get_take_nd_function(\n\u001b[0;32m    161\u001b[0m     arr\u001b[39m.\u001b[39mndim, arr\u001b[39m.\u001b[39mdtype, out\u001b[39m.\u001b[39mdtype, axis\u001b[39m=\u001b[39maxis, mask_info\u001b[39m=\u001b[39mmask_info\n\u001b[0;32m    162\u001b[0m )\n\u001b[0;32m    163\u001b[0m func(arr, indexer, out, fill_value)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 85.2 MiB for an array with shape (11166111, 1) and data type datetime64[ns]"
     ]
    }
   ],
   "source": [
    "\n",
    "# finally, 2022 filtering data - remove any rows that aren't from 2022\n",
    "data_2022_clean_drop['Start Date Time'] = pd.to_datetime(data_2022_clean_drop[\"Start Date\"], format=\"%d/%m/%Y %H:%M\")\n",
    "data_2022_clean_drop1 = data_2022_clean_drop[data_2022_clean_drop['Start Date Time'].dt.year == 2022]\n",
    "print(data_2022_clean_drop1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the additional column \n",
    "data_2022_clean_drop2 = data_2022_clean_drop1 .drop(['Start Date Time'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rental Id</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Bike Id</th>\n",
       "      <th>End Date</th>\n",
       "      <th>EndStation Id</th>\n",
       "      <th>EndStation Name</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>StartStation Id</th>\n",
       "      <th>StartStation Name</th>\n",
       "      <th>SS Terminal Name</th>\n",
       "      <th>ES Terminal Name</th>\n",
       "      <th>Bike model</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115967515.0</td>\n",
       "      <td>1260.000</td>\n",
       "      <td>15338.0</td>\n",
       "      <td>01/01/2022 23:13</td>\n",
       "      <td>310.0</td>\n",
       "      <td>Black Prince Road, Vauxhall</td>\n",
       "      <td>01/01/2022 22:52</td>\n",
       "      <td>529.0</td>\n",
       "      <td>Manresa Road, Chelsea</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116017034.0</td>\n",
       "      <td>720.000</td>\n",
       "      <td>19861.0</td>\n",
       "      <td>01/04/2022 19:08</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Brunswick Square, Bloomsbury</td>\n",
       "      <td>01/04/2022 18:56</td>\n",
       "      <td>804.0</td>\n",
       "      <td>Good's Way, King's Cross</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>116016563.0</td>\n",
       "      <td>480.000</td>\n",
       "      <td>19861.0</td>\n",
       "      <td>01/04/2022 18:46</td>\n",
       "      <td>804.0</td>\n",
       "      <td>Good's Way, King's Cross</td>\n",
       "      <td>01/04/2022 18:38</td>\n",
       "      <td>57.0</td>\n",
       "      <td>Guilford Street , Bloomsbury</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>116014412.0</td>\n",
       "      <td>1260.000</td>\n",
       "      <td>17235.0</td>\n",
       "      <td>01/04/2022 17:45</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Belgrove Street , King's Cross</td>\n",
       "      <td>01/04/2022 17:24</td>\n",
       "      <td>297.0</td>\n",
       "      <td>Geraldine Street, Elephant &amp; Castle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>116013350.0</td>\n",
       "      <td>480.000</td>\n",
       "      <td>13790.0</td>\n",
       "      <td>01/04/2022 16:50</td>\n",
       "      <td>252.0</td>\n",
       "      <td>Jubilee Gardens, South Bank</td>\n",
       "      <td>01/04/2022 16:42</td>\n",
       "      <td>310.0</td>\n",
       "      <td>Black Prince Road, Vauxhall</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11232176</th>\n",
       "      <td>127641458.0</td>\n",
       "      <td>6544.593</td>\n",
       "      <td>53664.0</td>\n",
       "      <td>26/12/2022 01:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Queen Mary's, Mile End</td>\n",
       "      <td>26/12/2022 00:02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Woodstock Grove, Shepherd's Bush</td>\n",
       "      <td>200214</td>\n",
       "      <td>200249</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11232177</th>\n",
       "      <td>127641459.0</td>\n",
       "      <td>1936.877</td>\n",
       "      <td>54303.0</td>\n",
       "      <td>26/12/2022 00:34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Salmon Lane, Limehouse</td>\n",
       "      <td>26/12/2022 00:02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Curlew Street, Shad Thames</td>\n",
       "      <td>1213</td>\n",
       "      <td>200147</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11232178</th>\n",
       "      <td>127641453.0</td>\n",
       "      <td>2955.280</td>\n",
       "      <td>21426.0</td>\n",
       "      <td>26/12/2022 00:49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Langdon Park, Poplar</td>\n",
       "      <td>26/12/2022 00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Curlew Street, Shad Thames</td>\n",
       "      <td>1213</td>\n",
       "      <td>200160</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11232179</th>\n",
       "      <td>127641454.0</td>\n",
       "      <td>5427.555</td>\n",
       "      <td>54786.0</td>\n",
       "      <td>26/12/2022 01:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Millharbour, Millwall</td>\n",
       "      <td>26/12/2022 00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Millharbour, Millwall</td>\n",
       "      <td>22167</td>\n",
       "      <td>22167</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11232180</th>\n",
       "      <td>127641455.0</td>\n",
       "      <td>2902.467</td>\n",
       "      <td>30605.0</td>\n",
       "      <td>26/12/2022 00:49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Langdon Park, Poplar</td>\n",
       "      <td>26/12/2022 00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Curlew Street, Shad Thames</td>\n",
       "      <td>1213</td>\n",
       "      <td>200160</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11166111 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Rental Id  Duration  Bike Id          End Date  EndStation Id  \\\n",
       "0         115967515.0  1260.000  15338.0  01/01/2022 23:13          310.0   \n",
       "1         116017034.0   720.000  19861.0  01/04/2022 19:08           11.0   \n",
       "3         116016563.0   480.000  19861.0  01/04/2022 18:46          804.0   \n",
       "4         116014412.0  1260.000  17235.0  01/04/2022 17:45           14.0   \n",
       "5         116013350.0   480.000  13790.0  01/04/2022 16:50          252.0   \n",
       "...               ...       ...      ...               ...            ...   \n",
       "11232176  127641458.0  6544.593  53664.0  26/12/2022 01:51            NaN   \n",
       "11232177  127641459.0  1936.877  54303.0  26/12/2022 00:34            NaN   \n",
       "11232178  127641453.0  2955.280  21426.0  26/12/2022 00:49            NaN   \n",
       "11232179  127641454.0  5427.555  54786.0  26/12/2022 01:31            NaN   \n",
       "11232180  127641455.0  2902.467  30605.0  26/12/2022 00:49            NaN   \n",
       "\n",
       "                         EndStation Name        Start Date  StartStation Id  \\\n",
       "0            Black Prince Road, Vauxhall  01/01/2022 22:52            529.0   \n",
       "1           Brunswick Square, Bloomsbury  01/04/2022 18:56            804.0   \n",
       "3               Good's Way, King's Cross  01/04/2022 18:38             57.0   \n",
       "4         Belgrove Street , King's Cross  01/04/2022 17:24            297.0   \n",
       "5            Jubilee Gardens, South Bank  01/04/2022 16:42            310.0   \n",
       "...                                  ...               ...              ...   \n",
       "11232176          Queen Mary's, Mile End  26/12/2022 00:02              NaN   \n",
       "11232177          Salmon Lane, Limehouse  26/12/2022 00:02              NaN   \n",
       "11232178            Langdon Park, Poplar  26/12/2022 00:00              NaN   \n",
       "11232179           Millharbour, Millwall  26/12/2022 00:00              NaN   \n",
       "11232180            Langdon Park, Poplar  26/12/2022 00:00              NaN   \n",
       "\n",
       "                            StartStation Name SS Terminal Name  \\\n",
       "0                       Manresa Road, Chelsea              NaN   \n",
       "1                    Good's Way, King's Cross              NaN   \n",
       "3                Guilford Street , Bloomsbury              NaN   \n",
       "4         Geraldine Street, Elephant & Castle              NaN   \n",
       "5                 Black Prince Road, Vauxhall              NaN   \n",
       "...                                       ...              ...   \n",
       "11232176     Woodstock Grove, Shepherd's Bush           200214   \n",
       "11232177           Curlew Street, Shad Thames             1213   \n",
       "11232178           Curlew Street, Shad Thames             1213   \n",
       "11232179                Millharbour, Millwall            22167   \n",
       "11232180           Curlew Street, Shad Thames             1213   \n",
       "\n",
       "         ES Terminal Name Bike model  Hour  Day  \n",
       "0                     NaN        NaN    22    5  \n",
       "1                     NaN        NaN    18    4  \n",
       "3                     NaN        NaN    18    4  \n",
       "4                     NaN        NaN    17    4  \n",
       "5                     NaN        NaN    16    4  \n",
       "...                   ...        ...   ...  ...  \n",
       "11232176           200249    CLASSIC     0    0  \n",
       "11232177           200147    CLASSIC     0    0  \n",
       "11232178           200160    CLASSIC     0    0  \n",
       "11232179            22167    CLASSIC     0    0  \n",
       "11232180           200160    CLASSIC     0    0  \n",
       "\n",
       "[11166111 rows x 14 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2022_clean_drop2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_data_2022 = data_2022_clean_drop2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the data in an PostgreSQL databse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psycopg2 library installed to connect to a PostgreSQL database from Python\n",
    "\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection to postgres database\n",
    "conn = psycopg2.connect(\n",
    "    user=\"postgres\",\n",
    "    password=\"password123\",\n",
    "    host=\"localhost\",\n",
    "    database=\"diss_data\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SQLAlchemy engine: Create a SQLAlchemy engine using the create_engine function, which will be used to write the DataFrame to the database.\n",
    "engine = create_engine('postgresql+psycopg2://postgres:password123@localhost:5432/diss_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bike_data_2019' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Export the DataFrame to the database: Once you have the connection and engine set up, you can use the to_sql method of the DataFrame to export it to the database.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# save the DataFrame to the PostgreSQL database\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# set the index parameter to False to avoid saving the DataFrame's index as a separate column in the database.\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m bike_data_2019\u001b[39m.\u001b[39mto_sql(\u001b[39m'\u001b[39m\u001b[39mbike_data_2019_tb\u001b[39m\u001b[39m'\u001b[39m, engine, if_exists\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mreplace\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bike_data_2019' is not defined"
     ]
    }
   ],
   "source": [
    "# Export the DataFrame to the database: Once you have the connection and engine set up, you can use the to_sql method of the DataFrame to export it to the database.\n",
    "# save the DataFrame to the PostgreSQL database\n",
    "# set the index parameter to False to avoid saving the DataFrame's index as a separate column in the database.\n",
    "bike_data_2019.to_sql('bike_data_2019_tb', engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the DataFrame to the PostgreSQL database\n",
    "bike_data_2022.to_sql('bike_data_2022_tb_v02', engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the dataframes into a matrix, whereby the value of each cell is the number of events per hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# connection to postgres database\n",
    "conn = psycopg2.connect(\n",
    "    user=\"postgres\",\n",
    "    password=\"password123\",\n",
    "    host=\"localhost\",\n",
    "    database=\"diss_data\",\n",
    ")\n",
    "\n",
    "engine = sqlalchemy.create_engine('postgresql://postgres:password123@localhost:5432/diss_data')\n",
    "\n",
    "# create a connection to the database\n",
    "conn = psycopg2.connect(database=\"diss_data\", user=\"postgres\", password=\"password123\", host=\"localhost\", port=\"5432\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EMoses\\AppData\\Local\\Temp\\ipykernel_2668\\229235612.py:5: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(sql_query, conn)\n"
     ]
    }
   ],
   "source": [
    "# define the SQL query to retrieve the data from the table\n",
    "sql_query = \"SELECT * FROM bike_data_2019_tb\"\n",
    "\n",
    "# use the read_sql function to read the table into a Pandas dataframe\n",
    "df = pd.read_sql(sql_query, conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EMoses\\AppData\\Local\\Temp\\ipykernel_2668\\2619909124.py:6: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_2022 = pd.read_sql(sql_query_2022, conn)\n"
     ]
    }
   ],
   "source": [
    "# doing ther same for 2022\n",
    "# define the SQL query to retrieve the data from the table\n",
    "sql_query_2022 = \"SELECT * FROM bike_data_2022_tb_v02\"\n",
    "\n",
    "# use the read_sql function to read the table into a Pandas dataframe\n",
    "df_2022 = pd.read_sql(sql_query_2022, conn)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets create a matrix for all the data in 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copying the dataframe\n",
    "bike_data_2019 = df.copy()\n",
    "bike_data_2022 = df_2022.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_station_names(station_names, df, namecolumn, idcolumn):\n",
    "    \"\"\"Given a DataFrame df that has df[namecolumn] listing names of stations\n",
    "    and df[idcolumn] listing station ID numbers, add to the dictionary\n",
    "    station_names all the names that each ID is attached to.\n",
    "\n",
    "    \"\"\"\n",
    "    namemaps = (\n",
    "        df[[idcolumn, namecolumn]]\n",
    "        .groupby(idcolumn)\n",
    "        .aggregate(lambda x: x.unique())\n",
    "    )\n",
    "    for number, names in namemaps.iterrows():\n",
    "        current_names = station_names.get(number, set())\n",
    "        # The following two lines are a stupid dance around the annoying fact\n",
    "        # that pd.unique sometimes returns a single value, sometimes a numpy\n",
    "        # array of values, but since the single value is a string, it too is an\n",
    "        # iterable.\n",
    "        vals = names[0]\n",
    "        new_names = set([vals]) if type(vals) == str else set(vals)\n",
    "        current_names.update(new_names)\n",
    "        station_names[number] = current_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_datetime_column(df, colname, roundto=\"H\"):\n",
    "    \"\"\"Parse df[colname] from strings to datetime objects, and round the times\n",
    "    to the nearest hour. \n",
    "    \"\"\"\n",
    "\n",
    "    format = \"%d/%m/%Y %H:%M\"\n",
    "    df.loc[:, colname] = pd.to_datetime(df[colname], format=format)\n",
    "    df.loc[:, colname] = df[colname].dt.round(roundto)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_single_events(df, which):\n",
    "    \"\"\"Read from df all the events, either departures or arrivals depending on\n",
    "    whether `which` is \"Start\" or \"End\", and collect them in a DataFrame that\n",
    "    lists event counts per station and time.\n",
    "    \"\"\"\n",
    "    stationcol = \"{}Station Id\".format(which)\n",
    "    datecol = \"{} Date\".format(which)\n",
    "    events = (\n",
    "        df.rename(columns={stationcol: \"Station\", datecol: \"Date\"})\n",
    "        .groupby([\"Date\", \"Station\"])\n",
    "        .size()\n",
    "        .unstack(\"Station\")\n",
    "    )\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_both_events(df):\n",
    "    \"\"\"Read from df all the events, both arrivals and departures, and collect\n",
    "    them in a DataFrame that lists event counts per station and time.\n",
    "    \"\"\"\n",
    "    arrivals = compute_single_events(df, \"End\")\n",
    "    departures = compute_single_events(df, \"Start\")\n",
    "    both = (\n",
    "        pd.concat(\n",
    "            [arrivals, departures], keys=[\"Arrivals\", \"Departures\"], axis=1\n",
    "        )\n",
    "        .reorder_levels([1, 0], axis=1)\n",
    "        .fillna(0.0)\n",
    "    )\n",
    "    return both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_allnames = {}\n",
    "add_station_names(station_allnames, bike_data_2019, \"EndStation Name\", \"EndStation Id\")\n",
    "add_station_names(station_allnames, bike_data_2019, \"StartStation Name\", \"StartStation Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'station_allnames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m station_allnames_newdic \u001b[39m=\u001b[39m {key: value\u001b[39m.\u001b[39mpop() \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m station_allnames\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(station_allnames_newdic)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'station_allnames' is not defined"
     ]
    }
   ],
   "source": [
    "station_allnames_newdic = {key: value.pop() for key, value in station_allnames.items()}\n",
    "\n",
    "print(station_allnames_newdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EMoses\\AppData\\Local\\Temp\\ipykernel_2668\\1812963310.py:11: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:, colname] = pd.to_datetime(df[colname], format=format)\n",
      "C:\\Users\\EMoses\\AppData\\Local\\Temp\\ipykernel_2668\\1812963310.py:12: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:, colname] = df[colname].dt.round(roundto)\n",
      "C:\\Users\\EMoses\\AppData\\Local\\Temp\\ipykernel_2668\\1812963310.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, colname] = pd.to_datetime(df[colname], format=format)\n",
      "C:\\Users\\EMoses\\AppData\\Local\\Temp\\ipykernel_2668\\1812963310.py:11: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:, colname] = pd.to_datetime(df[colname], format=format)\n",
      "C:\\Users\\EMoses\\AppData\\Local\\Temp\\ipykernel_2668\\1812963310.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, colname] = df[colname].dt.round(roundto)\n",
      "C:\\Users\\EMoses\\AppData\\Local\\Temp\\ipykernel_2668\\1812963310.py:12: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:, colname] = df[colname].dt.round(roundto)\n"
     ]
    }
   ],
   "source": [
    "#clean start and end dates\n",
    "bd_data_2019_clean1 = clean_datetime_column(bike_data_2019, \"Start Date\", roundto=\"H\")\n",
    "bd_data_2019_clean2 = clean_datetime_column(bd_data_2019_clean1, \"End Date\", roundto=\"H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rental Id</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Bike Id</th>\n",
       "      <th>End Date</th>\n",
       "      <th>EndStation Id</th>\n",
       "      <th>EndStation Name</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>StartStation Id</th>\n",
       "      <th>StartStation Name</th>\n",
       "      <th>Start station number</th>\n",
       "      <th>End station number</th>\n",
       "      <th>Bike model</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70045</th>\n",
       "      <td>115944485.0</td>\n",
       "      <td>780.000</td>\n",
       "      <td>17247.0</td>\n",
       "      <td>01/01/2022 00:00</td>\n",
       "      <td>804.0</td>\n",
       "      <td>Good's Way, King's Cross</td>\n",
       "      <td>2022-01-01 00:00:00</td>\n",
       "      <td>109.0</td>\n",
       "      <td>Soho Square , Soho</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97426</th>\n",
       "      <td>115944427.0</td>\n",
       "      <td>900.000</td>\n",
       "      <td>16287.0</td>\n",
       "      <td>01/01/2022 00:00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>Albert Embankment, Vauxhall</td>\n",
       "      <td>2022-01-01 00:00:00</td>\n",
       "      <td>653.0</td>\n",
       "      <td>Simpson Street, Clapham Junction</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105178</th>\n",
       "      <td>115943720.0</td>\n",
       "      <td>2520.000</td>\n",
       "      <td>6665.0</td>\n",
       "      <td>01/01/2022 00:00</td>\n",
       "      <td>213.0</td>\n",
       "      <td>Wellington Arch, Hyde Park</td>\n",
       "      <td>2021-12-31 23:00:00</td>\n",
       "      <td>839.0</td>\n",
       "      <td>Sea Containers, South Bank</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79304</th>\n",
       "      <td>115944542.0</td>\n",
       "      <td>660.000</td>\n",
       "      <td>21514.0</td>\n",
       "      <td>01/01/2022 00:00</td>\n",
       "      <td>163.0</td>\n",
       "      <td>Sloane Avenue, Knightsbridge</td>\n",
       "      <td>2022-01-01 00:00:00</td>\n",
       "      <td>169.0</td>\n",
       "      <td>Porchester Place, Paddington</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7682</th>\n",
       "      <td>115944488.0</td>\n",
       "      <td>780.000</td>\n",
       "      <td>17373.0</td>\n",
       "      <td>01/01/2022 00:00</td>\n",
       "      <td>804.0</td>\n",
       "      <td>Good's Way, King's Cross</td>\n",
       "      <td>2022-01-01 00:00:00</td>\n",
       "      <td>109.0</td>\n",
       "      <td>Soho Square , Soho</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11183500</th>\n",
       "      <td>127691861.0</td>\n",
       "      <td>992.086</td>\n",
       "      <td>53770.0</td>\n",
       "      <td>31/12/2022 23:57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lower Thames Street, Monument</td>\n",
       "      <td>2023-01-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pott Street, Bethnal Green</td>\n",
       "      <td>200156</td>\n",
       "      <td>1098</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11183944</th>\n",
       "      <td>127691406.0</td>\n",
       "      <td>3039.877</td>\n",
       "      <td>53091.0</td>\n",
       "      <td>31/12/2022 23:57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abbey Orchard Street, Westminster</td>\n",
       "      <td>2022-12-31 23:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Grosvenor Square, Mayfair</td>\n",
       "      <td>10627</td>\n",
       "      <td>3429</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11183599</th>\n",
       "      <td>127691782.0</td>\n",
       "      <td>1399.255</td>\n",
       "      <td>41612.0</td>\n",
       "      <td>31/12/2022 23:58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kennington Lane Rail Bridge, Vauxhall</td>\n",
       "      <td>2023-01-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sheepcote Lane, Battersea</td>\n",
       "      <td>200176</td>\n",
       "      <td>1190</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11183597</th>\n",
       "      <td>127691780.0</td>\n",
       "      <td>1395.675</td>\n",
       "      <td>41081.0</td>\n",
       "      <td>31/12/2022 23:58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kennington Lane Rail Bridge, Vauxhall</td>\n",
       "      <td>2023-01-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sheepcote Lane, Battersea</td>\n",
       "      <td>200176</td>\n",
       "      <td>1190</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11183372</th>\n",
       "      <td>127691998.0</td>\n",
       "      <td>612.267</td>\n",
       "      <td>50349.0</td>\n",
       "      <td>31/12/2022 23:59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lots Road, West Chelsea</td>\n",
       "      <td>2023-01-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Collingham Gardens, Earl's Court</td>\n",
       "      <td>3459</td>\n",
       "      <td>300076</td>\n",
       "      <td>CLASSIC</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11232181 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Rental Id  Duration  Bike Id          End Date  EndStation Id  \\\n",
       "70045     115944485.0   780.000  17247.0  01/01/2022 00:00          804.0   \n",
       "97426     115944427.0   900.000  16287.0  01/01/2022 00:00          100.0   \n",
       "105178    115943720.0  2520.000   6665.0  01/01/2022 00:00          213.0   \n",
       "79304     115944542.0   660.000  21514.0  01/01/2022 00:00          163.0   \n",
       "7682      115944488.0   780.000  17373.0  01/01/2022 00:00          804.0   \n",
       "...               ...       ...      ...               ...            ...   \n",
       "11183500  127691861.0   992.086  53770.0  31/12/2022 23:57            NaN   \n",
       "11183944  127691406.0  3039.877  53091.0  31/12/2022 23:57            NaN   \n",
       "11183599  127691782.0  1399.255  41612.0  31/12/2022 23:58            NaN   \n",
       "11183597  127691780.0  1395.675  41081.0  31/12/2022 23:58            NaN   \n",
       "11183372  127691998.0   612.267  50349.0  31/12/2022 23:59            NaN   \n",
       "\n",
       "                                EndStation Name          Start Date  \\\n",
       "70045                  Good's Way, King's Cross 2022-01-01 00:00:00   \n",
       "97426               Albert Embankment, Vauxhall 2022-01-01 00:00:00   \n",
       "105178               Wellington Arch, Hyde Park 2021-12-31 23:00:00   \n",
       "79304              Sloane Avenue, Knightsbridge 2022-01-01 00:00:00   \n",
       "7682                   Good's Way, King's Cross 2022-01-01 00:00:00   \n",
       "...                                         ...                 ...   \n",
       "11183500          Lower Thames Street, Monument 2023-01-01 00:00:00   \n",
       "11183944      Abbey Orchard Street, Westminster 2022-12-31 23:00:00   \n",
       "11183599  Kennington Lane Rail Bridge, Vauxhall 2023-01-01 00:00:00   \n",
       "11183597  Kennington Lane Rail Bridge, Vauxhall 2023-01-01 00:00:00   \n",
       "11183372                Lots Road, West Chelsea 2023-01-01 00:00:00   \n",
       "\n",
       "          StartStation Id                 StartStation Name  \\\n",
       "70045               109.0                Soho Square , Soho   \n",
       "97426               653.0  Simpson Street, Clapham Junction   \n",
       "105178              839.0        Sea Containers, South Bank   \n",
       "79304               169.0      Porchester Place, Paddington   \n",
       "7682                109.0                Soho Square , Soho   \n",
       "...                   ...                               ...   \n",
       "11183500              NaN        Pott Street, Bethnal Green   \n",
       "11183944              NaN         Grosvenor Square, Mayfair   \n",
       "11183599              NaN         Sheepcote Lane, Battersea   \n",
       "11183597              NaN         Sheepcote Lane, Battersea   \n",
       "11183372              NaN  Collingham Gardens, Earl's Court   \n",
       "\n",
       "         Start station number End station number Bike model  Hour  Day  \n",
       "70045                    None               None       None    23    4  \n",
       "97426                    None               None       None    23    4  \n",
       "105178                   None               None       None    23    4  \n",
       "79304                    None               None       None    23    4  \n",
       "7682                     None               None       None    23    4  \n",
       "...                       ...                ...        ...   ...  ...  \n",
       "11183500               200156               1098    CLASSIC    23    5  \n",
       "11183944                10627               3429    CLASSIC    23    5  \n",
       "11183599               200176               1190    CLASSIC    23    5  \n",
       "11183597               200176               1190    CLASSIC    23    5  \n",
       "11183372                 3459             300076    CLASSIC    23    5  \n",
       "\n",
       "[11232181 rows x 14 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_data_2022.sort_values(by=\"End Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EMoses\\AppData\\Local\\Temp\\ipykernel_2668\\2632925062.py:9: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:, colname] = pd.to_datetime(df[colname], format=format)\n",
      "C:\\Users\\EMoses\\AppData\\Local\\Temp\\ipykernel_2668\\2632925062.py:10: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:, colname] = df[colname].dt.round(roundto)\n",
      "C:\\Users\\EMoses\\AppData\\Local\\Temp\\ipykernel_2668\\2632925062.py:9: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:, colname] = pd.to_datetime(df[colname], format=format)\n",
      "C:\\Users\\EMoses\\AppData\\Local\\Temp\\ipykernel_2668\\2632925062.py:10: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:, colname] = df[colname].dt.round(roundto)\n"
     ]
    }
   ],
   "source": [
    "#clean start and end dates\n",
    "bd_data_2022_clean1 = clean_datetime_column(bike_data_2022, \"Start Date\", roundto=\"H\")\n",
    "bd_data_2022_clean2 = clean_datetime_column(bd_data_2022_clean1, \"End Date\", roundto=\"H\")\n",
    "bd_data_2022_clean2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 42.8 MiB for an array with shape (11232181,) and data type int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m bd_data_2022_clean2\u001b[39m.\u001b[39;49msort_values(by\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mStart Date\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\core\\frame.py:6923\u001b[0m, in \u001b[0;36mDataFrame.sort_values\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   6920\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(ascending, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m)):\n\u001b[0;32m   6921\u001b[0m         ascending \u001b[39m=\u001b[39m ascending[\u001b[39m0\u001b[39m]\n\u001b[1;32m-> 6923\u001b[0m     indexer \u001b[39m=\u001b[39m nargsort(\n\u001b[0;32m   6924\u001b[0m         k, kind\u001b[39m=\u001b[39;49mkind, ascending\u001b[39m=\u001b[39;49mascending, na_position\u001b[39m=\u001b[39;49mna_position, key\u001b[39m=\u001b[39;49mkey\n\u001b[0;32m   6925\u001b[0m     )\n\u001b[0;32m   6926\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   6927\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\EMoses\\miniconda3\\envs\\diss\\lib\\site-packages\\pandas\\core\\sorting.py:432\u001b[0m, in \u001b[0;36mnargsort\u001b[1;34m(items, kind, ascending, na_position, key, mask)\u001b[0m\n\u001b[0;32m    430\u001b[0m idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39mlen\u001b[39m(items))\n\u001b[0;32m    431\u001b[0m non_nans \u001b[39m=\u001b[39m items[\u001b[39m~\u001b[39mmask]\n\u001b[1;32m--> 432\u001b[0m non_nan_idx \u001b[39m=\u001b[39m idx[\u001b[39m~\u001b[39;49mmask]\n\u001b[0;32m    434\u001b[0m nan_idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnonzero(mask)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ascending:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 42.8 MiB for an array with shape (11232181,) and data type int32"
     ]
    }
   ],
   "source": [
    "bd_data_2022_clean2.sort_values(by=\"Start Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_data_2019 = compute_both_events(bd_data_2019_clean2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_data_2022 = compute_both_events(bd_data_2022_clean2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally rename the columns according to the chosen names for stations.\n",
    "events_2019 = events_data_2019.rename(mapper=station_allnames_newdic, axis=1, level=0)\n",
    "events_2019 = events_2019.sort_index(axis=1, level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Station</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>829</th>\n",
       "      <th>830</th>\n",
       "      <th>831</th>\n",
       "      <th>832</th>\n",
       "      <th>833</th>\n",
       "      <th>834</th>\n",
       "      <th>835</th>\n",
       "      <th>836</th>\n",
       "      <th>838</th>\n",
       "      <th>839</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Arrivals</th>\n",
       "      <th>Arrivals</th>\n",
       "      <th>Arrivals</th>\n",
       "      <th>Arrivals</th>\n",
       "      <th>Arrivals</th>\n",
       "      <th>Arrivals</th>\n",
       "      <th>Arrivals</th>\n",
       "      <th>Arrivals</th>\n",
       "      <th>Arrivals</th>\n",
       "      <th>Arrivals</th>\n",
       "      <th>...</th>\n",
       "      <th>Departures</th>\n",
       "      <th>Departures</th>\n",
       "      <th>Departures</th>\n",
       "      <th>Departures</th>\n",
       "      <th>Departures</th>\n",
       "      <th>Departures</th>\n",
       "      <th>Departures</th>\n",
       "      <th>Departures</th>\n",
       "      <th>Departures</th>\n",
       "      <th>Departures</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 01:00:00</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 02:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 03:00:00</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 04:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 19:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 20:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 22:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 23:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8759 rows × 1594 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Station                  1        2        3        4        5        6    \\\n",
       "                    Arrivals Arrivals Arrivals Arrivals Arrivals Arrivals   \n",
       "Date                                                                        \n",
       "2019-01-01 00:00:00      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2019-01-01 01:00:00      1.0      0.0      0.0      5.0      1.0      0.0   \n",
       "2019-01-01 02:00:00      0.0      0.0      5.0      2.0      1.0      0.0   \n",
       "2019-01-01 03:00:00      1.0      1.0      2.0      0.0      2.0      1.0   \n",
       "2019-01-01 04:00:00      0.0      0.0      0.0      2.0      0.0      0.0   \n",
       "...                      ...      ...      ...      ...      ...      ...   \n",
       "2019-12-31 19:00:00      0.0      2.0      0.0      0.0      3.0      0.0   \n",
       "2019-12-31 20:00:00      0.0      0.0      0.0      0.0      1.0      0.0   \n",
       "2019-12-31 21:00:00      0.0      3.0      0.0      0.0      0.0      1.0   \n",
       "2019-12-31 22:00:00      0.0      0.0      0.0      0.0      1.0      0.0   \n",
       "2019-12-31 23:00:00      0.0      0.0      0.0      0.0      2.0      0.0   \n",
       "\n",
       "Station                  7        8        9        10   ...        829  \\\n",
       "                    Arrivals Arrivals Arrivals Arrivals  ... Departures   \n",
       "Date                                                     ...              \n",
       "2019-01-01 00:00:00      0.0      2.0      0.0      4.0  ...        0.0   \n",
       "2019-01-01 01:00:00      0.0      2.0      0.0      1.0  ...        0.0   \n",
       "2019-01-01 02:00:00      2.0      0.0      0.0      0.0  ...        0.0   \n",
       "2019-01-01 03:00:00      1.0      1.0      0.0      0.0  ...        0.0   \n",
       "2019-01-01 04:00:00      1.0      0.0      0.0      0.0  ...        0.0   \n",
       "...                      ...      ...      ...      ...  ...        ...   \n",
       "2019-12-31 19:00:00      1.0      1.0      3.0      0.0  ...        1.0   \n",
       "2019-12-31 20:00:00      3.0      2.0      0.0      0.0  ...        0.0   \n",
       "2019-12-31 21:00:00      0.0      0.0      4.0      0.0  ...        0.0   \n",
       "2019-12-31 22:00:00      0.0      2.0      0.0      0.0  ...        0.0   \n",
       "2019-12-31 23:00:00      3.0      0.0      0.0      0.0  ...        0.0   \n",
       "\n",
       "Station                    830        831        832        833        834  \\\n",
       "                    Departures Departures Departures Departures Departures   \n",
       "Date                                                                         \n",
       "2019-01-01 00:00:00        1.0        0.0        0.0        0.0        0.0   \n",
       "2019-01-01 01:00:00        0.0        0.0        0.0        0.0        0.0   \n",
       "2019-01-01 02:00:00        0.0        0.0        0.0        1.0        0.0   \n",
       "2019-01-01 03:00:00        0.0        7.0        0.0        0.0        0.0   \n",
       "2019-01-01 04:00:00        1.0        0.0        0.0        4.0        0.0   \n",
       "...                        ...        ...        ...        ...        ...   \n",
       "2019-12-31 19:00:00        0.0        0.0        0.0        1.0        1.0   \n",
       "2019-12-31 20:00:00        0.0        0.0        0.0        0.0        1.0   \n",
       "2019-12-31 21:00:00        0.0        0.0        1.0        0.0        0.0   \n",
       "2019-12-31 22:00:00        0.0        0.0        0.0        0.0        0.0   \n",
       "2019-12-31 23:00:00        0.0        2.0        2.0        0.0        0.0   \n",
       "\n",
       "Station                    835        836        838        839  \n",
       "                    Departures Departures Departures Departures  \n",
       "Date                                                             \n",
       "2019-01-01 00:00:00        0.0        0.0        0.0        0.0  \n",
       "2019-01-01 01:00:00        1.0        0.0        0.0        0.0  \n",
       "2019-01-01 02:00:00        1.0        1.0        2.0        0.0  \n",
       "2019-01-01 03:00:00        0.0        3.0        0.0        0.0  \n",
       "2019-01-01 04:00:00        0.0        3.0        0.0        0.0  \n",
       "...                        ...        ...        ...        ...  \n",
       "2019-12-31 19:00:00        0.0        0.0        0.0        3.0  \n",
       "2019-12-31 20:00:00        0.0        2.0        0.0        1.0  \n",
       "2019-12-31 21:00:00        2.0        0.0        0.0        0.0  \n",
       "2019-12-31 22:00:00        0.0        0.0        0.0        0.0  \n",
       "2019-12-31 23:00:00        0.0        0.0        0.0        0.0  \n",
       "\n",
       "[8759 rows x 1594 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_data_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Station</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>3.0</th>\n",
       "      <th>4.0</th>\n",
       "      <th>5.0</th>\n",
       "      <th>6.0</th>\n",
       "      <th>7.0</th>\n",
       "      <th>8.0</th>\n",
       "      <th>9.0</th>\n",
       "      <th>10.0</th>\n",
       "      <th>...</th>\n",
       "      <th>836.0</th>\n",
       "      <th>838.0</th>\n",
       "      <th>839.0</th>\n",
       "      <th>840.0</th>\n",
       "      <th>841.0</th>\n",
       "      <th>842.0</th>\n",
       "      <th>844.0</th>\n",
       "      <th>845.0</th>\n",
       "      <th>846.0</th>\n",
       "      <th>850.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Arrivals</th>\n",
       "      <th>Arrivals</th>\n",
       "      <th>Arrivals</th>\n",
       "      <th>Arrivals</th>\n",
       "      <th>Arrivals</th>\n",
       "      <th>Arrivals</th>\n",
       "      <th>Arrivals</th>\n",
       "      <th>Arrivals</th>\n",
       "      <th>Arrivals</th>\n",
       "      <th>Arrivals</th>\n",
       "      <th>...</th>\n",
       "      <th>Departures</th>\n",
       "      <th>Departures</th>\n",
       "      <th>Departures</th>\n",
       "      <th>Departures</th>\n",
       "      <th>Departures</th>\n",
       "      <th>Departures</th>\n",
       "      <th>Departures</th>\n",
       "      <th>Departures</th>\n",
       "      <th>Departures</th>\n",
       "      <th>Departures</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-12-29 00:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-29 01:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-29 02:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-29 03:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-29 04:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-09 09:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-09 10:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-09 15:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-09 18:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-09 21:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6151 rows × 1602 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Station                1.0      2.0      3.0      4.0      5.0      6.0    \\\n",
       "                    Arrivals Arrivals Arrivals Arrivals Arrivals Arrivals   \n",
       "Date                                                                        \n",
       "2021-12-29 00:00:00      0.0      0.0      0.0      1.0      0.0      0.0   \n",
       "2021-12-29 01:00:00      0.0      1.0      0.0      0.0      0.0      0.0   \n",
       "2021-12-29 02:00:00      0.0      1.0      0.0      0.0      0.0      0.0   \n",
       "2021-12-29 03:00:00      0.0      0.0      0.0      0.0      0.0      1.0   \n",
       "2021-12-29 04:00:00      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "...                      ...      ...      ...      ...      ...      ...   \n",
       "2022-12-09 09:00:00      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2022-12-09 10:00:00      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2022-12-09 15:00:00      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2022-12-09 18:00:00      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2022-12-09 21:00:00      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "Station                7.0      8.0      9.0      10.0   ...      836.0  \\\n",
       "                    Arrivals Arrivals Arrivals Arrivals  ... Departures   \n",
       "Date                                                     ...              \n",
       "2021-12-29 00:00:00      0.0      0.0      0.0      0.0  ...        0.0   \n",
       "2021-12-29 01:00:00      0.0      0.0      0.0      1.0  ...        0.0   \n",
       "2021-12-29 02:00:00      0.0      1.0      0.0      1.0  ...        0.0   \n",
       "2021-12-29 03:00:00      0.0      0.0      0.0      0.0  ...        0.0   \n",
       "2021-12-29 04:00:00      0.0      0.0      1.0      0.0  ...        0.0   \n",
       "...                      ...      ...      ...      ...  ...        ...   \n",
       "2022-12-09 09:00:00      0.0      0.0      0.0      0.0  ...        0.0   \n",
       "2022-12-09 10:00:00      0.0      0.0      0.0      0.0  ...        0.0   \n",
       "2022-12-09 15:00:00      0.0      0.0      0.0      0.0  ...        0.0   \n",
       "2022-12-09 18:00:00      0.0      0.0      0.0      0.0  ...        0.0   \n",
       "2022-12-09 21:00:00      0.0      0.0      0.0      0.0  ...        0.0   \n",
       "\n",
       "Station                  838.0      839.0      840.0      841.0      842.0  \\\n",
       "                    Departures Departures Departures Departures Departures   \n",
       "Date                                                                         \n",
       "2021-12-29 00:00:00        0.0        4.0        0.0        0.0        2.0   \n",
       "2021-12-29 01:00:00        0.0        0.0        0.0        0.0        2.0   \n",
       "2021-12-29 02:00:00        0.0        0.0        2.0        2.0        0.0   \n",
       "2021-12-29 03:00:00        0.0        0.0        0.0        0.0        0.0   \n",
       "2021-12-29 04:00:00        0.0        0.0        0.0        0.0        0.0   \n",
       "...                        ...        ...        ...        ...        ...   \n",
       "2022-12-09 09:00:00        0.0        0.0        0.0        0.0        0.0   \n",
       "2022-12-09 10:00:00        0.0        0.0        0.0        0.0        0.0   \n",
       "2022-12-09 15:00:00        0.0        0.0        0.0        0.0        0.0   \n",
       "2022-12-09 18:00:00        0.0        0.0        0.0        0.0        0.0   \n",
       "2022-12-09 21:00:00        0.0        0.0        0.0        0.0        0.0   \n",
       "\n",
       "Station                  844.0      845.0      846.0      850.0  \n",
       "                    Departures Departures Departures Departures  \n",
       "Date                                                             \n",
       "2021-12-29 00:00:00        0.0        0.0        0.0        0.0  \n",
       "2021-12-29 01:00:00        0.0        0.0        0.0        0.0  \n",
       "2021-12-29 02:00:00        0.0        0.0        0.0        0.0  \n",
       "2021-12-29 03:00:00        0.0        0.0        0.0        0.0  \n",
       "2021-12-29 04:00:00        0.0        0.0        0.0        0.0  \n",
       "...                        ...        ...        ...        ...  \n",
       "2022-12-09 09:00:00        0.0        0.0        0.0        0.0  \n",
       "2022-12-09 10:00:00        0.0        0.0        0.0        0.0  \n",
       "2022-12-09 15:00:00        0.0        0.0        0.0        0.0  \n",
       "2022-12-09 18:00:00        0.0        0.0        0.0        0.0  \n",
       "2022-12-09 21:00:00        0.0        0.0        0.0        0.0  \n",
       "\n",
       "[6151 rows x 1602 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_data_2022"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting the the events data frame as a pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "events_path = Path(\"data/events_2019.p\")\n",
    "\n",
    "# Store the file on disk so we can read it later.\n",
    "events_2019.to_pickle(events_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "249cc518374e28bf1fa10a0460f1501eab88ca4f883968af225d88dc54cdfb38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
